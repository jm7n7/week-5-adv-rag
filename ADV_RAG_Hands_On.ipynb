{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "15YQ7diZ7uHEFaCl-oKB9jtDZXsw8w-nq",
      "authorship_tag": "ABX9TyPRyTj1dSqLbBLpl6uxpBju",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jm7n7/week-5-adv-rag/blob/main/ADV_RAG_Hands_On.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Track A, B, C: Reranking & Context Optimization | Multimodal RAG | Evaluation & Guardrails**"
      ],
      "metadata": {
        "id": "wCQWk7dibv4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Install & Setup\n",
        "*   Install\n",
        "    - numpy\n",
        "    - pandas\n",
        "    - matplotlib\n",
        "    - sentence-transformers\n",
        "    - faiss\n",
        "    - langchain\n",
        "    - openai\n",
        "*   Log environment to env_rag_adv.json\n",
        "##2. Load Your Project Materials\n",
        "*   Use the same documents from week 4 (optional add new documents)\n",
        "    - PDFs (research papers, survey articles, datasets)\n",
        "    - Text/Markdown notes\n",
        "*   Include 2-3 images / charts for Track B\n",
        "##3. Retrieval Upgrades (Track A)\n",
        "*   Implement RRF (BM25 + dense)\n",
        "    - Add reranker + compression\n",
        "*   Log\n",
        "    - Recall@k\n",
        "    - latency\n",
        "    - avg context length\n",
        "    - token cost\n",
        "##4. Multimodal Retrieval (Track B)\n",
        "*   Caption / encode images with CLIP/BLIP2/Gemini-Vision\n",
        "*   Show at least _one image-only query_ retrieving a relevant chart with citations\n",
        "##5. Evaluation & Guardrails (Track C)\n",
        "*   Build eval_queries.jsonl\n",
        "*   Compute\n",
        "    - correctness/faithfullness\n",
        "    - latency before guardrails\n",
        "    - latency after guardrails\n",
        "*   Include at least _one adversarial/unsafe/PII query_ to test guardrails\n",
        "##6. Ablation Study\n",
        "*   Fill ablation_results.csv:\n",
        "    - Baseline\n",
        "    - +Rerank\n",
        "    - +Compression\n",
        "    - +Multimodal\n",
        "    - +Guardrails\n",
        "*   Plot recall versus latency using matplotlib\n",
        "##7. Reproducibility log\n",
        "*   Save configs in rag_adv_run_config.json\n",
        "    - embedding models\n",
        "    - reranker\n",
        "    - chunking\n",
        "    - multimodal pipeline\n",
        "    - guardrails\n",
        "    - retriever (k)"
      ],
      "metadata": {
        "id": "ogq5iHOYcFZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1"
      ],
      "metadata": {
        "id": "Sc9S96QKiVFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install\n",
        "%pip install langchain chromadb sentence-transformers transformers langchain-community pypdf"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rsnmQWYCiSAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import sys\n",
        "import platform\n",
        "import transformers\n",
        "import sentence_transformers\n",
        "import chromadb\n",
        "import json\n",
        "import os\n",
        "try:\n",
        "    import torch\n",
        "    torch_v = torch.__version__\n",
        "    cuda_ok = torch.cuda.is_available()\n",
        "    device_name = torch.cuda.get_device_name(0) if cuda_ok else \"CPU\"\n",
        "except:\n",
        "    torch_v, cuda_ok, device_name = \"N/A\", False, \"CPU\""
      ],
      "metadata": {
        "id": "XWGR8zoni9wX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b0a4d52"
      },
      "source": [
        "# Log versions\n",
        "env_info = {\n",
        "    \"python\": sys.version,\n",
        "    \"platform\": platform.platform(),\n",
        "    \"torch\": torch_v,\n",
        "    \"cuda\": cuda_ok,\n",
        "    \"device\": device_name,\n",
        "    \"transformers\": transformers.__version__,\n",
        "    \"sentence_transformers\": sentence_transformers.__version__,\n",
        "    \"chromadb\": chromadb.__version__\n",
        "}\n",
        "\n",
        "# Save results in env_rag.json\n",
        "output_dir = '/content/drive/MyDrive/Capstone/Week 4_RAG'\n",
        "file_path = os.path.join(output_dir, \"env_rag.json\")\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "# Check if the file exists and load existing data\n",
        "existing_data = {}\n",
        "if os.path.exists(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            existing_data = json.load(f)\n",
        "    except json.JSONDecodeError:\n",
        "        existing_data = {} # Handle empty or invalid JSON\n",
        "\n",
        "# Update existing data with new environment info\n",
        "existing_data.update(env_info)\n",
        "\n",
        "with open(file_path, 'w') as f:\n",
        "    json.dump(existing_data, f, indent=4)\n",
        "\n",
        "print(f\"Environment information saved to {file_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2"
      ],
      "metadata": {
        "id": "gyblUWHuj6RW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90cbc9d4"
      },
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Define the directory where the PDF files are located\n",
        "pdf_dir = '/content/drive/MyDrive/Capstone/Week 4_RAG'\n",
        "\n",
        "# List of PDF files to load\n",
        "pdf_files = [\"maia-2.pdf\", \"Amortized_chess.pdf\", \"chessgpt.pdf\"]\n",
        "\n",
        "# Load the documents\n",
        "documents = []\n",
        "for pdf_file in pdf_files:\n",
        "    file_path = os.path.join(pdf_dir, pdf_file)\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    documents.extend(loader.load())\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dea875ae"
      },
      "source": [
        "## Step 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00be2770"
      },
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Define chunking parameters\n",
        "chunk_size = 500\n",
        "chunk_overlap = 100\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap\n",
        ")\n",
        "\n",
        "# Split the documents into chunks\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# Preview chunk count and first chunk\n",
        "print(f\"Created {len(chunks)} chunks.\")\n",
        "if chunks:\n",
        "    print(\"\\nFirst chunk:\")\n",
        "    print(chunks[0].page_content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file path\n",
        "file_path = os.path.join(output_dir, \"rag_run_config.json\")\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "# Check if the file exists and load existing data\n",
        "existing_data = {}\n",
        "if os.path.exists(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            existing_data = json.load(f)\n",
        "    except json.JSONDecodeError:\n",
        "        existing_data = {} # Handle empty or invalid JSON\n",
        "\n",
        "# Update existing data with chunk parameters\n",
        "existing_data.update({\n",
        "    \"chunk_size\": chunk_size,\n",
        "    \"chunk_overlap\": chunk_overlap\n",
        "})\n",
        "\n",
        "# Save the updated data to the file\n",
        "with open(file_path, 'w') as f:\n",
        "    json.dump(existing_data, f, indent=4)\n",
        "\n",
        "print(f\"Chunk parameters saved to {file_path}\")"
      ],
      "metadata": {
        "id": "9xpubacPl3kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a913682"
      },
      "source": [
        "## Step 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "646701f2"
      },
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings.sentence_transformer import (\n",
        "    SentenceTransformerEmbeddings,\n",
        ")\n",
        "\n",
        "# Initialize the embedding model\n",
        "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "# Create the Chroma vector database\n",
        "# We'll store the database in the same output directory\n",
        "db_dir = os.path.join(output_dir, \"chroma_db\")\n",
        "vectorstore = Chroma.from_documents(chunks, embedding_function, persist_directory=db_dir)\n",
        "\n",
        "# Create a retriever from the vector store\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# Verify retrieval with a sample query\n",
        "sample_query = \"What is the main idea of the Maia-2 paper?\"\n",
        "docs = retriever.invoke(sample_query)\n",
        "\n",
        "print(f\"\\nSample Query: {sample_query}\")\n",
        "print(f\"\\nRetrieved {len(docs)} documents:\")\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content)\n",
        "\n",
        "# Save embedding model and retriever k value to rag_run_config.json\n",
        "file_path = os.path.join(output_dir, \"rag_run_config.json\")\n",
        "\n",
        "# Check if the file exists and load existing data\n",
        "existing_data = {}\n",
        "if os.path.exists(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            existing_data = json.load(f)\n",
        "    except json.JSONDecodeError:\n",
        "        existing_data = {} # Handle empty or invalid JSON\n",
        "\n",
        "# Update existing data with new information\n",
        "existing_data.update({\n",
        "    \"embedding_model\": embedding_model_name,\n",
        "    \"retriever_k\": 4\n",
        "})\n",
        "\n",
        "# Save the updated data to the file\n",
        "with open(file_path, 'w') as f:\n",
        "    json.dump(existing_data, f, indent=4)\n",
        "\n",
        "print(f\"\\nConfiguration updated in {file_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41ed7887"
      },
      "source": [
        "## Step 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c7f1aad"
      },
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Define the model to use (e.g., TinyLlama or distilgpt2)\n",
        "# Make sure to choose a model that fits within your computational resources\n",
        "model_id = \"distilgpt2\" # Changing to a different model\n",
        "task = \"text-generation\" # Update task for text generation models\n",
        "\n",
        "# Get the Hugging Face API token from Colab secrets\n",
        "# Make sure you have added your token to Colab secrets with the name 'HF_TOKEN'\n",
        "huggingface_api_token = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "# Initialize the Hugging Face LLM\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=model_id,\n",
        "    task=task, # Use the updated task\n",
        "    huggingfacehub_api_token=huggingface_api_token,\n",
        ")\n",
        "\n",
        "print(f\"Connected to Hugging Face model: {model_id}\")\n",
        "\n",
        "# Note: You might need to install the 'huggingface_hub' library if not already installed\n",
        "# %pip install huggingface_hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7db89eb3"
      },
      "source": [
        "## Step 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f104244"
      },
      "source": [
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# Define the model to use (using the same model as before)\n",
        "model_id = \"distilgpt2\"\n",
        "task = \"text-generation\"\n",
        "\n",
        "# Create a Hugging Face pipeline\n",
        "pipe = pipeline(task, model=model_id)\n",
        "\n",
        "# Initialize the LangChain LLM with the pipeline\n",
        "llm_pipeline = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "print(f\"Initialized LLM using HuggingFacePipeline with model: {model_id}\")\n",
        "\n",
        "# Now, you can use 'llm_pipeline' in your RetrievalQA chain\n",
        "# I will modify the next cell to use this new LLM object."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51bafdec"
      },
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Create a RetrievalQA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm_pipeline, # Use the llm_pipeline object\n",
        "    chain_type=\"stuff\", # Other options include \"map_reduce\", \"refine\", \"map_rerank\"\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True # Set to True to see the source documents\n",
        ")\n",
        "\n",
        "# Ask three domain-specific question\n",
        "query_1 = \"What is the main idea of the Maia-2 paper?\" # Same as first question\n",
        "query_2 = \"What is the conclusion of the Maia-2 paper?\" # Related to first question\n",
        "query_3 = \"What models were used in the chessGPT paper?\" # Brand new qestion context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the query_1\n",
        "result = qa_chain.invoke(query_1)\n",
        "\n",
        "print(f\"Query: {query_1}\")\n",
        "print(f\"\\nAnswer: {result['result']}\")\n",
        "\n",
        "# Optionally print source documents\n",
        "if 'source_documents' in result:\n",
        "    print(\"\\nSource Documents:\")\n",
        "    for i, doc in enumerate(result['source_documents']):\n",
        "        print(f\"\\nDocument {i+1}:\")\n",
        "        print(f\"Content: {doc.page_content[:200]}...\") # Print first 200 characters\n",
        "        print(f\"Source: {doc.metadata.get('source')}\")"
      ],
      "metadata": {
        "id": "blDtqJTck2AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the query_2\n",
        "result = qa_chain.invoke(query_2)\n",
        "\n",
        "print(f\"Query: {query_2}\")\n",
        "print(f\"\\nAnswer: {result['result']}\")\n",
        "\n",
        "# Optionally print source documents\n",
        "if 'source_documents' in result:\n",
        "    print(\"\\nSource Documents:\")\n",
        "    for i, doc in enumerate(result['source_documents']):\n",
        "        print(f\"\\nDocument {i+1}:\")\n",
        "        print(f\"Content: {doc.page_content[:200]}...\") # Print first 200 characters\n",
        "        print(f\"Source: {doc.metadata.get('source')}\")"
      ],
      "metadata": {
        "id": "w8vQKNSAlAJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the query_3\n",
        "result = qa_chain.invoke(query_3)\n",
        "\n",
        "print(f\"Query: {query_3}\")\n",
        "print(f\"\\nAnswer: {result['result']}\")\n",
        "\n",
        "# Optionally print source documents\n",
        "if 'source_documents' in result:\n",
        "    print(\"\\nSource Documents:\")\n",
        "    for i, doc in enumerate(result['source_documents']):\n",
        "        print(f\"\\nDocument {i+1}:\")\n",
        "        print(f\"Content: {doc.page_content[:200]}...\") # Print first 200 characters\n",
        "        print(f\"Source: {doc.metadata.get('source')}\")"
      ],
      "metadata": {
        "id": "Iln1PgKrlCPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03001945"
      },
      "source": [
        "## Step 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b32133ee"
      },
      "source": [
        "from langchain_community.embeddings.sentence_transformer import (\n",
        "    SentenceTransformerEmbeddings,\n",
        ")\n",
        "\n",
        "# Initialize a new embedding function\n",
        "embedding_model_name_e5 = \"intfloat/e5-small-v2\"\n",
        "embedding_function_e5 = SentenceTransformerEmbeddings(model_name=embedding_model_name_e5)\n",
        "\n",
        "# Create a new Chroma vector database\n",
        "db_dir_e5 = os.path.join(output_dir, f\"chroma_db_{embedding_model_name_e5.replace('-', '_')}\") # Update db_dir name\n",
        "vectorstore_e5 = Chroma.from_documents(chunks, embedding_function_e5, persist_directory=db_dir_e5)\n",
        "\n",
        "# Create a new retriever\n",
        "retriever_e5 = vectorstore_e5.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# Create a new RetrievalQA chain instance\n",
        "qa_chain_e5 = RetrievalQA.from_chain_type(\n",
        "    llm=llm_pipeline,  # Use the same LLM pipeline\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever_e5,\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the same three domain-specific queries\n",
        "query_1 = \"What is the main idea of the Maia-2 paper?\"\n",
        "query_2 = \"What is the conclusion of the Maia-2 paper?\"\n",
        "query_3 = \"What models were used in the chessGPT paper?\"\n",
        "\n",
        "queries = [query_1, query_2, query_3]\n",
        "results_e5 = {}\n",
        "\n",
        "# Invoke the new RetrievalQA chain and print results\n",
        "print(f\"\\n--- Results with {embedding_model_name_e5} embeddings ---\")\n",
        "for i, query in enumerate(queries):\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    result_e5 = qa_chain_e5.invoke(query)\n",
        "    print(f\"\\nAnswer: {result_e5['result']}\")\n",
        "\n",
        "    results_e5[f\"query_{i+1}\"] = {\n",
        "        \"query\": query,\n",
        "        \"answer\": result_e5['result'],\n",
        "        \"source_documents\": [{\"content\": doc.page_content, \"source\": doc.metadata.get('source')} for doc in result_e5['source_documents']]\n",
        "    }\n",
        "\n",
        "    if 'source_documents' in result_e5:\n",
        "        print(\"\\nSource Documents:\")\n",
        "        for j, doc in enumerate(result_e5['source_documents']):\n",
        "            print(f\"\\nDocument {j+1}:\")\n",
        "            print(f\"Content: {doc.page_content[:200]}...\")\n",
        "            print(f\"Source: {doc.metadata.get('source')}\")"
      ],
      "metadata": {
        "id": "HXL7NZRsuhBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the existing data from rag_run_config.json\n",
        "file_path = os.path.join(output_dir, \"rag_run_config.json\")\n",
        "existing_data = {}\n",
        "if os.path.exists(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            existing_data = json.load(f)\n",
        "    except json.JSONDecodeError:\n",
        "        existing_data = {} # Handle empty or invalid JSON\n",
        "\n",
        "# Update the loaded data\n",
        "existing_data[f\"embedding_experiment_{embedding_model_name_e5.replace('-', '_')}\"] = {\n",
        "    \"embedding_model\": embedding_model_name_e5,\n",
        "    \"retriever_k\": 4,\n",
        "    \"results\": results_e5\n",
        "}\n",
        "\n",
        "# Save the updated data to the file\n",
        "with open(file_path, 'w') as f:\n",
        "    json.dump(existing_data, f, indent=4)\n",
        "\n",
        "print(f\"\\nConfiguration updated with {embedding_model_name_e5} results in {file_path}\")"
      ],
      "metadata": {
        "id": "o4BqO4udvRQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd4f472c"
      },
      "source": [
        "# Define new chunking parameters\n",
        "new_chunk_size = 300\n",
        "new_chunk_overlap = 50\n",
        "\n",
        "print(f\"Performing Chunk Sensitivity Experiment with chunk_size={new_chunk_size} and chunk_overlap={new_chunk_overlap}\")\n",
        "\n",
        "# Initialize the text splitter with new parameters\n",
        "text_splitter_new = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=new_chunk_size,\n",
        "    chunk_overlap=new_chunk_overlap\n",
        ")\n",
        "\n",
        "# Split the original documents into chunks using new parameters\n",
        "chunks_new = text_splitter_new.split_documents(documents) # Using the 'documents' variable from Step 2\n",
        "\n",
        "print(f\"Created {len(chunks_new)} new chunks.\")\n",
        "\n",
        "# Initialize the same embedding model used in Step 4\n",
        "embedding_model_name_original = \"all-MiniLM-L6-v2\"\n",
        "embedding_function_original = SentenceTransformerEmbeddings(model_name=embedding_model_name_original)\n",
        "\n",
        "# Create a new Chroma vector database with new chunks and the original embedding model\n",
        "db_dir_new_chunks = os.path.join(output_dir, f\"chroma_db_chunk_{new_chunk_size}_{new_chunk_overlap}\")\n",
        "vectorstore_new_chunks = Chroma.from_documents(chunks_new, embedding_function_original, persist_directory=db_dir_new_chunks)\n",
        "\n",
        "# Create a new retriever from this vector store\n",
        "retriever_new_chunks = vectorstore_new_chunks.as_retriever(search_kwargs={\"k\": 4}) # Using the same k as before\n",
        "\n",
        "# Create a new RetrievalQA chain instance\n",
        "qa_chain_new_chunks = RetrievalQA.from_chain_type(\n",
        "    llm=llm_pipeline,  # Use the same LLM pipeline from Step 6\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever_new_chunks,\n",
        "    return_source_documents=True\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the same three domain-specific queries from Step 6\n",
        "query_1 = \"What is the main idea of the Maia-2 paper?\"\n",
        "query_2 = \"What is the conclusion of the Maia-2 paper?\"\n",
        "query_3 = \"What models were used in the chessGPT paper?\"\n",
        "\n",
        "queries = [query_1, query_2, query_3]\n",
        "results_new_chunks = {}\n",
        "\n",
        "# Invoke the new RetrievalQA chain and print results\n",
        "print(f\"\\n--- Results with chunk_size={new_chunk_size}, chunk_overlap={new_chunk_overlap} ---\")\n",
        "for i, query in enumerate(queries):\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    result_new_chunks = qa_chain_new_chunks.invoke(query)\n",
        "    print(f\"\\nAnswer: {result_new_chunks['result']}\")\n",
        "\n",
        "    results_new_chunks[f\"query_{i+1}\"] = {\n",
        "        \"query\": query,\n",
        "        \"answer\": result_new_chunks['result'],\n",
        "        \"source_documents\": [{\"content\": doc.page_content, \"source\": doc.metadata.get('source')} for doc in result_new_chunks['source_documents']]\n",
        "    }\n",
        "\n",
        "    if 'source_documents' in result_new_chunks:\n",
        "        print(\"\\nSource Documents:\")\n",
        "        for j, doc in enumerate(result_new_chunks['source_documents']):\n",
        "            print(f\"\\nDocument {j+1}:\")\n",
        "            print(f\"Content: {doc.page_content[:200]}...\")\n",
        "            print(f\"Source: {doc.metadata.get('source')}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7pi8UBJhwZLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the existing data from rag_run_config.json\n",
        "file_path = os.path.join(output_dir, \"rag_run_config.json\")\n",
        "existing_data = {}\n",
        "if os.path.exists(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            existing_data = json.load(f)\n",
        "    except json.JSONDecodeError:\n",
        "        existing_data = {} # Handle empty or invalid JSON\n",
        "\n",
        "# Update the loaded data with new chunking experiment results\n",
        "existing_data[f\"chunk_experiment_{new_chunk_size}_{new_chunk_overlap}\"] = {\n",
        "    \"chunk_size\": new_chunk_size,\n",
        "    \"chunk_overlap\": new_chunk_overlap,\n",
        "    \"embedding_model\": embedding_model_name_original,\n",
        "    \"retriever_k\": 4,\n",
        "    \"results\": results_new_chunks\n",
        "}\n",
        "\n",
        "# Save the updated data to the file\n",
        "with open(file_path, 'w') as f:\n",
        "    json.dump(existing_data, f, indent=4)\n",
        "\n",
        "print(f\"\\nConfiguration updated with chunk_size={new_chunk_size}, chunk_overlap={new_chunk_overlap} results in {file_path}\")"
      ],
      "metadata": {
        "id": "cNkFy9xJwcer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8\n",
        "*Skipped*"
      ],
      "metadata": {
        "id": "SeDnQ6r-xV07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9"
      ],
      "metadata": {
        "id": "TsFIGYPpxaRE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad25ab3f"
      },
      "source": [
        "file_path = os.path.join(output_dir, \"rag_run_config.json\")\n",
        "\n",
        "# Load the data from the file\n",
        "try:\n",
        "    with open(file_path, 'r') as f:\n",
        "        config_data = json.load(f)\n",
        "except (FileNotFoundError, json.JSONDecodeError):\n",
        "    print(f\"Error loading data from {file_path}\")\n",
        "    config_data = {} # Initialize empty if file not found or invalid\n",
        "\n",
        "print(\"--- Experiment Summary ---\")\n",
        "\n",
        "# Summarize Embedding Swap Experiment\n",
        "embedding_experiment_key = None\n",
        "for key in config_data:\n",
        "    if key.startswith(\"embedding_experiment_\"):\n",
        "        embedding_experiment_key = key\n",
        "        break\n",
        "\n",
        "if embedding_experiment_key:\n",
        "    embedding_experiment_data = config_data[embedding_experiment_key]\n",
        "    original_embedding_model = config_data.get(\"embedding_model\", \"N/A\") # Get original embedding model\n",
        "    print(f\"\\nEmbedding Swap Experiment:\")\n",
        "    print(f\"  Original Embedding Model: {original_embedding_model}\")\n",
        "    print(f\"  Compared Against: {embedding_experiment_data.get('embedding_model', 'N/A')}\")\n",
        "    print(\"  Review the 'results' section in rag_run_config.json for detailed output.\")\n",
        "else:\n",
        "    print(\"\\nEmbedding Swap Experiment data not found in rag_run_config.json\")\n",
        "\n",
        "\n",
        "# Summarize Chunk Sensitivity Experiment\n",
        "chunk_experiment_key = None\n",
        "for key in config_data:\n",
        "    if key.startswith(\"chunk_experiment_\"):\n",
        "        chunk_experiment_key = key\n",
        "        break\n",
        "\n",
        "if chunk_experiment_key:\n",
        "    chunk_experiment_data = config_data[chunk_experiment_key]\n",
        "    original_chunk_size = config_data.get(\"chunk_size\", \"N/A\")\n",
        "    original_chunk_overlap = config_data.get(\"chunk_overlap\", \"N/A\")\n",
        "    print(f\"\\nChunk Sensitivity Experiment:\")\n",
        "    print(f\"  Original Chunk Settings: chunk_size={original_chunk_size}, chunk_overlap={original_chunk_overlap}\")\n",
        "    print(f\"  Compared Against: chunk_size={chunk_experiment_data.get('chunk_size', 'N/A')}, chunk_overlap={chunk_experiment_data.get('chunk_overlap', 'N/A')}\")\n",
        "    print(\"  Review the 'results' section in rag_run_config.json for detailed output.\")\n",
        "else:\n",
        "    print(\"\\nChunk Sensitivity Experiment data not found in rag_run_config.json\")\n",
        "\n",
        "print(\"\\n--- End of Summary ---\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}