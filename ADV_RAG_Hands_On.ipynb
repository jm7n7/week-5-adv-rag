{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1qPKTFP_NDB4O6l9hsH9vq9XLgzWrdL6L",
      "authorship_tag": "ABX9TyOlnEUFZOup+GHHiD9faAwA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jm7n7/week-5-adv-rag/blob/main/ADV_RAG_Hands_On.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Track A, B, C: Reranking & Context Optimization | Multimodal RAG | Evaluation & Guardrails**"
      ],
      "metadata": {
        "id": "wCQWk7dibv4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Track A: Reranking & Context Optimization\n",
        "*   Implement Reciprocal Rank Fusion (BM25 + dense)\n",
        "*   Add a reranker (e.g., cross-encoder) and/or MMR/passage compression\n",
        "*   Compare Baseline vs. +Rerank vs. +Compression vs. +Both on your project queries\n",
        "##Track B: Multimodal RAG\n",
        "*  Add image/chart support via captions/embeddings (e.g., Gemini-Vision, BLIP2)\n",
        "*   Build a joint index for text + image\n",
        "*  Demonstrate text-only, image-only, and hybrid queries with grounded citations\n",
        "##Track C: Evaluation & Guardrails\n",
        "*   Create an eval set (10-20 project queries) with gold answers + source IDs\n",
        "*   Compute correctness, faithfullness, context precision/recall, latency, token cost\n",
        "*   Add guardrails: citation enforcement, PII redaction, refusal template\n",
        "*   Report Before vs. after guardrails"
      ],
      "metadata": {
        "id": "gnQn_ZmGxBQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Install & Setup\n",
        "*   Install\n",
        "    - numpy\n",
        "    - pandas\n",
        "    - matplotlib\n",
        "    - sentence-transformers\n",
        "    - faiss\n",
        "    - langchain\n",
        "    - openai\n",
        "*   Log environment to env_rag_adv.json\n",
        "##2. Load Your Project Materials\n",
        "*   Use the same documents from week 4 (optional add new documents)\n",
        "    - PDFs (research papers, survey articles, datasets)\n",
        "    - Text/Markdown notes\n",
        "*   Include 2-3 images / charts for Track B\n",
        "##3. Retrieval Upgrades (Track A)\n",
        "*   Implement RRF (BM25 + dense)\n",
        "    - Add reranker + compression\n",
        "*   Log\n",
        "    - Recall@k\n",
        "    - latency\n",
        "    - avg context length\n",
        "    - token cost\n",
        "##4. Multimodal Retrieval (Track B)\n",
        "*   Caption / encode images with CLIP/BLIP2/Gemini-Vision\n",
        "*   Show at least _one image-only query_ retrieving a relevant chart with citations\n",
        "##5. Evaluation & Guardrails (Track C)\n",
        "*   Build eval_queries.jsonl\n",
        "*   Compute\n",
        "    - correctness/faithfullness\n",
        "    - latency before guardrails\n",
        "    - latency after guardrails\n",
        "*   Include at least _one adversarial/unsafe/PII query_ to test guardrails\n",
        "##6. Ablation Study\n",
        "*   Fill ablation_results.csv:\n",
        "    - Baseline\n",
        "    - +Rerank\n",
        "    - +Compression\n",
        "    - +Multimodal\n",
        "    - +Guardrails\n",
        "*   Plot recall versus latency using matplotlib\n",
        "##7. Reproducibility log\n",
        "*   Save configs in rag_adv_run_config.json\n",
        "    - embedding models\n",
        "    - reranker\n",
        "    - chunking\n",
        "    - multimodal pipeline\n",
        "    - guardrails\n",
        "    - retriever (k)"
      ],
      "metadata": {
        "id": "ogq5iHOYcFZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1"
      ],
      "metadata": {
        "id": "Sc9S96QKiVFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install\n",
        "%pip install langchain chromadb sentence-transformers transformers langchain-community pypdf rank_bm25"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rsnmQWYCiSAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c514b366-920f-4aad-d11d-fd1e5f72a678"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.1.0)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.12/dist-packages (0.2.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.28)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.17.4)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "import platform\n",
        "import chromadb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import sentence_transformers\n",
        "import matplotlib.pyplot as plt\n",
        "#\n",
        "from rank_bm25 import BM25Okapi\n",
        "from google.colab import userdata\n",
        "from langchain.schema import Document\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings.sentence_transformer import (\n",
        "    SentenceTransformerEmbeddings,\n",
        ")\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
        "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "#\n",
        "try:\n",
        "    import torch\n",
        "    torch_v = torch.__version__\n",
        "    cuda_ok = torch.cuda.is_available()\n",
        "    device_name = torch.cuda.get_device_name(0) if cuda_ok else \"CPU\"\n",
        "except:\n",
        "    torch_v, cuda_ok, device_name = \"N/A\", False, \"CPU\""
      ],
      "metadata": {
        "id": "XWGR8zoni9wX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b0a4d52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41aa376f-dc7e-48d4-9a79-1280e359e9b2"
      },
      "source": [
        "# Log versions\n",
        "env_info = {\n",
        "    \"python\": sys.version,\n",
        "    \"platform\": platform.platform(),\n",
        "    \"torch\": torch_v,\n",
        "    \"cuda\": cuda_ok,\n",
        "    \"device\": device_name,\n",
        "    \"transformers\": transformers.__version__,\n",
        "    \"sentence_transformers\": sentence_transformers.__version__,\n",
        "    \"chromadb\": chromadb.__version__,\n",
        "    \"numpy\": np.__version__,\n",
        "    \"pandas\": pd.__version__,\n",
        "}\n",
        "\n",
        "# Save results in env_rag_adv.json\n",
        "output_dir = '/content/drive/MyDrive/Capstone/Week 5_Advanced_RAG'\n",
        "file_path = os.path.join(output_dir, \"env_rag_adv.json\")\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "# Check if the file exists and load existing data\n",
        "existing_data = {}\n",
        "if os.path.exists(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            existing_data = json.load(f)\n",
        "    except json.JSONDecodeError:\n",
        "        existing_data = {} # Handle empty or invalid JSON\n",
        "\n",
        "# Update existing data with new environment info\n",
        "existing_data.update(env_info)\n",
        "\n",
        "with open(file_path, 'w') as f:\n",
        "    json.dump(existing_data, f, indent=4)\n",
        "\n",
        "print(f\"Environment information saved to {file_path}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment information saved to /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/env_rag_adv.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2"
      ],
      "metadata": {
        "id": "gyblUWHuj6RW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90cbc9d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628a6d12-7d5a-4c39-efec-92621a4cf8e8"
      },
      "source": [
        "# Define the directory where the files are located\n",
        "file_dir = '/content/drive/MyDrive/Capstone/Week 5_Advanced_RAG'\n",
        "\n",
        "# List of PDF files to load for text-based RAG\n",
        "pdf_files = [\"maia-2.pdf\", \"Amortized_chess.pdf\", \"chessgpt.pdf\"]\n",
        "\n",
        "# List of PNG files to be used for multimodal RAG (will be processed separately)\n",
        "png_files = [\"daily_puzzle.png\", \"puzzle_1.png\", \"puzzle_2.png\",\"puzzle_3.png\"]\n",
        "\n",
        "# Load the PDF documents using PyPDFLoader\n",
        "pdf_documents = []\n",
        "for pdf_file in pdf_files:\n",
        "    file_path = os.path.join(file_dir, pdf_file)\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    pdf_documents.extend(loader.load())\n",
        "\n",
        "png_documents = []\n",
        "for png_file in png_files:\n",
        "    file_path = os.path.join(file_dir, png_file)\n",
        "    # Create a simple Document object with file path as content and source\n",
        "    png_documents.append(Document(page_content=f\"Image file: {png_file}\", metadata={\"source\": file_path, \"file_type\": \"png\"}))\n",
        "\n",
        "# Combine all documents (PDFs and placeholder PNGs)\n",
        "all_documents = pdf_documents + png_documents\n",
        "\n",
        "print(f\"Loaded {len(pdf_documents)} PDF documents.\")\n",
        "print(f\"Listed and created placeholder documents for {len(png_documents)} PNG files for future multimodal processing.\")\n",
        "print(f\"Total documents (PDFs + PNG placeholders): {len(all_documents)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 99 PDF documents.\n",
            "Listed and created placeholder documents for 4 PNG files for future multimodal processing.\n",
            "Total documents (PDFs + PNG placeholders): 103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dea875ae"
      },
      "source": [
        "## Step 3 (Track A)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replicate Week 4 work"
      ],
      "metadata": {
        "id": "OMF-VbZe_5sW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00be2770",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb16c3b-dded-43b6-9e2a-856edc8aa6b5"
      },
      "source": [
        "# Define chunking parameters\n",
        "chunk_size = 500\n",
        "chunk_overlap = 100\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap\n",
        ")\n",
        "\n",
        "# Split the documents into chunks\n",
        "chunks = text_splitter.split_documents(all_documents) # Use 'all_documents' from Step 2\n",
        "\n",
        "# Preview chunk count and first chunk\n",
        "print(f\"Created {len(chunks)} chunks.\")\n",
        "if chunks:\n",
        "    print(\"\\nFirst chunk:\")\n",
        "    print(chunks[0].page_content)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 813 chunks.\n",
            "\n",
            "First chunk:\n",
            "Maia-2: A Unified Model for Human-AI Alignment in\n",
            "Chess\n",
            "Zhenwei Tang\n",
            "University of Toronto\n",
            "josephtang@cs.toronto.edu\n",
            "Difan Jiao\n",
            "University of Toronto\n",
            "difanjiao@cs.toronto.edu\n",
            "Reid McIlroy-Young\n",
            "Harvard University\n",
            "reidmcy@seas.harvard.edu\n",
            "Jon Kleinberg\n",
            "Cornell University\n",
            "kleinberg@cornell.edu\n",
            "Siddhartha Sen\n",
            "Microsoft Research\n",
            "sidsen@microsoft.com\n",
            "Ashton Anderson\n",
            "University of Toronto\n",
            "ashton@cs.toronto.edu\n",
            "Abstract\n",
            "There are an increasing number of domains in which artificial intelligence (AI)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the embedding model\n",
        "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "# Create the Chroma vector database\n",
        "# We'll store the database in the same output directory\n",
        "db_dir = os.path.join(output_dir, \"chroma_db\")\n",
        "vectorstore = Chroma.from_documents(chunks, embedding_function, persist_directory=db_dir)\n",
        "\n",
        "# Create a retriever from the vector store\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# Verify retrieval with a sample query\n",
        "sample_query = \"What is the main idea of the Maia-2 paper?\"\n",
        "docs = retriever.invoke(sample_query)\n",
        "\n",
        "print(f\"\\nSample Query: {sample_query}\")\n",
        "print(f\"\\nRetrieved {len(docs)} documents:\")\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content)\n",
        "\n",
        "# Save embedding model and retriever k value to rag_run_config.json\n",
        "file_path = os.path.join(output_dir, \"rag_run_config.json\")\n",
        "\n",
        "# Check if the file exists and load existing data\n",
        "existing_data = {}\n",
        "if os.path.exists(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            existing_data = json.load(f)\n",
        "    except json.JSONDecodeError:\n",
        "        existing_data = {} # Handle empty or invalid JSON\n",
        "\n",
        "# Update existing data with new information\n",
        "existing_data.update({\n",
        "    \"embedding_model\": embedding_model_name,\n",
        "    \"retriever_k\": 4\n",
        "})\n",
        "\n",
        "# Save the updated data to the file\n",
        "with open(file_path, 'w') as f:\n",
        "    json.dump(existing_data, f, indent=4)\n",
        "\n",
        "print(f\"\\nConfiguration updated in {file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SaUa81U8tLj",
        "outputId": "951a165b-751d-4576-ab62-9caaed69f08f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3652338492.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_function = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Query: What is the main idea of the Maia-2 paper?\n",
            "\n",
            "Retrieved 4 documents:\n",
            "\n",
            "Document 1:\n",
            "interact with chess positions to produce the moves humans make. Unlike previous models, Maia-2\n",
            "only requires the current board position as input (as opposed to six), which dramatically reduces\n",
            "training time and increases flexibility (e.g. for applying the model in non-game contexts where there\n",
            "may be no 6-board history). In addition to policy and value heads like in previous work, we also add\n",
            "an additional auxiliary information head that helps the model learn a deeper understanding of human\n",
            "\n",
            "Document 2:\n",
            "interact with chess positions to produce the moves humans make. Unlike previous models, Maia-2\n",
            "only requires the current board position as input (as opposed to six), which dramatically reduces\n",
            "training time and increases flexibility (e.g. for applying the model in non-game contexts where there\n",
            "may be no 6-board history). In addition to policy and value heads like in previous work, we also add\n",
            "an additional auxiliary information head that helps the model learn a deeper understanding of human\n",
            "\n",
            "Document 3:\n",
            "interact with chess positions to produce the moves humans make. Unlike previous models, Maia-2\n",
            "only requires the current board position as input (as opposed to six), which dramatically reduces\n",
            "training time and increases flexibility (e.g. for applying the model in non-game contexts where there\n",
            "may be no 6-board history). In addition to policy and value heads like in previous work, we also add\n",
            "an additional auxiliary information head that helps the model learn a deeper understanding of human\n",
            "\n",
            "Document 4:\n",
            "interact with chess positions to produce the moves humans make. Unlike previous models, Maia-2\n",
            "only requires the current board position as input (as opposed to six), which dramatically reduces\n",
            "training time and increases flexibility (e.g. for applying the model in non-game contexts where there\n",
            "may be no 6-board history). In addition to policy and value heads like in previous work, we also add\n",
            "an additional auxiliary information head that helps the model learn a deeper understanding of human\n",
            "\n",
            "Configuration updated in /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/rag_run_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Week 5 upgrades"
      ],
      "metadata": {
        "id": "XRazebdaCCHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of document texts for BM25\n",
        "document_texts = [doc.page_content for doc in pdf_documents]\n",
        "\n",
        "# Initialize BM25Retriever\n",
        "bm25_retriever = BM25Retriever.from_texts(document_texts, metadatas=[doc.metadata for doc in pdf_documents])\n",
        "bm25_retriever.k = 4 # Set a default k value for BM25\n",
        "\n",
        "print(\"BM25 Retriever initialized.\")\n",
        "\n",
        "# Use the existing vectorstore for the dense retriever\n",
        "dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4}) # Set k for dense\n",
        "\n",
        "print(\"Dense Retriever initialized.\")\n",
        "\n",
        "# Initialize the EnsembleRetriever with BM25 and dense retrievers\n",
        "# weights can be adjusted based on desired contribution of each retriever\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, dense_retriever], weights=[0.5, 0.5])\n",
        "\n",
        "print(\"Ensemble Retriever (RRF) initialized.\")\n",
        "\n",
        "# Test the RRF retriever with a sample query\n",
        "sample_query = \"What is the main idea of the Maia-2 paper?\"\n",
        "docs_rrf = ensemble_retriever.invoke(sample_query)\n",
        "\n",
        "print(f\"\\nSample Query with RRF: {sample_query}\")\n",
        "print(f\"\\nRetrieved {len(docs_rrf)} documents using RRF:\")\n",
        "for i, doc in enumerate(docs_rrf):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content)\n",
        "    print(f\"Source: {doc.metadata.get('source')}\") # Include source information if available"
      ],
      "metadata": {
        "id": "mbD3GPdB-L2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "5e144fc2-b4e0-4ca7-fac2-37e1fce33aba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 Retriever initialized.\n",
            "Dense Retriever initialized.\n",
            "Ensemble Retriever (RRF) initialized.\n",
            "\n",
            "Sample Query with RRF: What is the main idea of the Maia-2 paper?\n",
            "\n",
            "Retrieved 5 documents using RRF:\n",
            "\n",
            "Document 1:\n",
            "interact with chess positions to produce the moves humans make. Unlike previous models, Maia-2\n",
            "only requires the current board position as input (as opposed to six), which dramatically reduces\n",
            "training time and increases flexibility (e.g. for applying the model in non-game contexts where there\n",
            "may be no 6-board history). In addition to policy and value heads like in previous work, we also add\n",
            "an additional auxiliary information head that helps the model learn a deeper understanding of human\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 2:\n",
            "Figure 4: Maia-2’s chess concept recognition as a function of skill level, as measured by linear\n",
            "activation probes right before (blue) and after (orange) skill-aware attention. (a) Stockfish overall\n",
            "board evaluation for middle-game positions. (b) Stockfish evaluation of middle-game bonuses and\n",
            "penalties to pieces for white. (c) Does the active player own two bishops? (d) Can the active player\n",
            "capture the opponent’s queen?\n",
            "subtle nuances. We now turn our focus to a critical question: does Maia-2 vary in its ability to capture\n",
            "human chess concepts when given different skill levels? Following the chess concepts probing\n",
            "strategy for AlphaZero [22], we show how Maia-2’s grasp of various concepts varies with skill. The\n",
            "left two plots in Figure 4 show concepts for which Maia-2 clearly distinguishes between skill levels,\n",
            "with higher-skill players paying more attention to them than lower-skill players. These are general\n",
            "board evaluations as given by Stockfish [ 6], or aggregate piece values. Note that pre-skill-aware\n",
            "attention is always flat because by construction it cannot vary with skill, since skill-aware attention has\n",
            "not been applied yet. The two plots on the right depict concepts that live closer to fundamental chess\n",
            "rules, and as such are less dependent on player skill. For skill-dependent concepts, the figures reveal\n",
            "an increasing trend in mastery level after skill-aware attention, aligning with the increase in dedicated\n",
            "skill levels. Meanwhile, the model’s mastery level decreases after passing through the skill-aware\n",
            "attention modules, potentially adjusting for the imperfections of human players. Conversely, the\n",
            "skill-aware attention blocks are not responsive to skill-independent concepts.\n",
            "5 Discussion\n",
            "Human Study. In addition to human move matching, we also consider engagement, another\n",
            "dimension of human study. In particular, we implement a randomized experiment on Lichess:\n",
            "human players challenge our bots, and we randomize whether players play against Maia-1 or Maia-2.\n",
            "Our result is that our higher move-matching and our vastly improved coherence, across all skill\n",
            "levels, come at no cost to human subject engagement, and in fact slightly increase engagement:\n",
            "players rematch Maia-2 almost 1 percentage point more than Maia-1 (41.2% vs. 40.3%). Although\n",
            "engagement is not our main objective, this is further promising evidence that we have achieved a\n",
            "more human-aligned model that coherently captures human style across different skill levels.\n",
            "Ethical Considerations. We believe Maia-2 poses limited risk while offering large potential benefits.\n",
            "Our data is highly aggregated, with almost 1 billion games being used for training, and chess as a\n",
            "domain is generally low-risk. Meanwhile, helping people improve in chess could lead to increased\n",
            "cognitive skills, confidence boosts, and help with general life satisfaction. Our vision is for Maia-2 to\n",
            "power AI partners and training aids; it cannot currently replace skilled human tutors and coaches.\n",
            "Limitation. Our work has limitations. First, we are excited by the applications that Maia-2 will\n",
            "enable, such as more relatable AI partners and AI-powered learning aids, the development of which is\n",
            "out of scope for the current work. Maia-2 does not yet incorporate search, although previous work has\n",
            "demonstrated that with proper regularization it can help improve move prediction performance [26].\n",
            "Relatedly, we group the strongest players in a single bucket, although modeling the very best players\n",
            "in the world remains difficult due to the complexity and depth of their moves.\n",
            "10\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 3:\n",
            "1200 1600 2000\n",
            "Active Player's Skill Level\n",
            "1200 1600 2000\n",
            "Opponent Player's Skill Level\n",
            "48.0 47.5 47.3 46.1\n",
            "49.2 48.4 48.8 48.1 47.5\n",
            "49.5 49.3 49.0 49.1 48.4 47.8\n",
            "50.1 50.0 49.9 49.8 49.2 48.7 48.6\n",
            "50.4 50.8 50.6 50.4 50.1 50.1 49.6\n",
            "51.0 50.9 51.5 50.9 51.0 50.7 50.8\n",
            "50.2 50.5 51.5 51.6 51.5 51.5 51.4\n",
            "50.5 51.9 51.6 52.0 50.8 51.8\n",
            "51.5 51.9 52.2 52.2 50.6\n",
            "51.6 51.8 52.0 52.8\n",
            "Maia 1100\n",
            "1200 1600 2000\n",
            "Active Player's Skill Level\n",
            "51.1 52.0 52.5 52.9\n",
            "51.1 51.5 52.8 52.9 53.3\n",
            "49.4 51.2 51.6 52.9 53.0 54.0\n",
            "49.0 49.9 51.4 52.0 52.7 53.5 54.4\n",
            "47.8 49.5 50.9 51.6 52.2 53.3 54.2\n",
            "47.5 48.3 50.4 50.5 51.8 52.9 53.9\n",
            "46.2 47.3 49.6 50.3 50.7 52.0 53.0\n",
            "46.3 48.7 49.2 50.3 49.8 52.3\n",
            "47.7 48.4 49.4 50.2 49.0\n",
            "47.3 48.3 48.8 50.2\n",
            "Maia 1900\n",
            "1200 1600 2000\n",
            "Active Player's Skill Level\n",
            "52.4 53.1 53.8 53.6\n",
            "52.7 53.3 54.2 54.2 54.3\n",
            "51.5 53.0 53.2 54.3 54.5 55.0\n",
            "51.6 52.2 53.4 53.8 54.4 54.9 55.7\n",
            "51.1 52.1 53.1 53.7 54.2 55.2 55.6\n",
            "51.1 51.5 53.4 53.1 54.0 54.8 55.6\n",
            "50.0 50.6 52.8 53.4 53.3 54.5 55.1\n",
            "50.2 52.2 52.6 53.4 52.3 54.8\n",
            "51.4 52.0 52.9 53.3 52.2\n",
            "51.2 52.1 52.4 53.9\n",
            "Maia-2\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "Move Prediction Accuracy\n",
            "Figure 2: Move prediction accuracy across diverse skill levels. Colors represent performance, with\n",
            "warmer tones indicating higher accuracy.\n",
            "4.1 Move Prediction Accuracy\n",
            "Maia-2. In Table 1, we show the top-1 move prediction accuracy of all models across all groups of\n",
            "players on the Maia-1 Testset. Maia-2 demonstrates strong and consistent performance across all\n",
            "skill levels, surpassing all baselines. Specifically, despite Maia-1 models being specifically trained to\n",
            "mimic chess moves by players at specific skill levels, Maia-2 emerges as a unified one-for-all model\n",
            "that is consistently effective across the entire spectrum of chess skills. The largest improvement is\n",
            "on Advanced players, where Maia-2 gains 1.5 percentage points over the nearest competitor (Maia\n",
            "1500). When averaging across skill levels, Maia-2 outperforms all other models by almost 2 full\n",
            "percentage points in overall accuracy. Note that the ceiling accuracy of human move prediction is far\n",
            "below 100% given the randomness and diversity of human decisions—even thesame player won’t\n",
            "always make the same decision when faced with the same position. Our 2 percentage point gain is\n",
            "substantial considering that the difference between Maia-1 and Leela, the previous state-of-the-art\n",
            "model for this task and a traditional chess engine not trained for this task at all, is only 6 percentage\n",
            "points. Furthermore, Maia-1 is essentially a mixture of 9 experts targeting the specific players’\n",
            "skill level, where each expert has 10.3M parameters. Regarding the routing function to select the\n",
            "best-performing expert as a nonparameterized function, Maia-1 has 92M parameters in total. Maia-2,\n",
            "on the other hand, is a one-for-all model with 23.3M parameters under our default settings. Therefore,\n",
            "Maia-2 achieves better human move prediction accuracy with even much fewer trainable parameters.\n",
            "Baseline models. Both Maia-2 and Maia-1 significantly outperform Stockfish and Leela, typically by\n",
            "5–15 percentage points. Note that Stockfish and Leela aim to play optimal chess (as most humans do\n",
            "too), and only “predict” human moves when their approximations to optimality happen to overlap\n",
            "with those of human players. However, we compare to these traditional chess engines because besides\n",
            "Maia-1, there are still the default method of creating “human-like” AI agents. The accuracy gap\n",
            "between Maia-1 architectures and traditional chess engines demonstrates the necessity of developing\n",
            "specialized models to mimic human chess moves.\n",
            "Maia-2subset. Maia-2 differs from Maia-1 in two main ways: it has a different architecture and it\n",
            "has access to more training data. To control for the difference in training data and isolate the effects\n",
            "of our architecture, we create Maia-2 subset which has access to the exact same training data that\n",
            "Maia-1 was developed with. Comparing the two, we see that Maia-2subset matches or outperforms\n",
            "all baselines and alternate models. Recall that Maia-2 and Maia-2subset don’t have the recent history\n",
            "passed as input to them, yet still achieve state-of-the-art results. It is important to note that each\n",
            "Maia-1 model is specifically trained for its respective skill level, relying solely on games where the\n",
            "active and opponent skill levels match for its training data. On the contrary, the unified modeling\n",
            "approach with skill-aware attention of Maia-2subset allows it to utilize a broader spectrum of games,\n",
            "featuring a variety of skill-level pairings, for training purposes. Consequently, while both Maia-1 and\n",
            "Maia-2subset draw from the same source dataset, Maia-2subset can leverage a significantly larger portion\n",
            "of this data for its training, improving its learning and predictive capabilities. The improvement from\n",
            "Maia-2subset to Maia-2 underscores the importance of extensive training with vast datasets. A broader\n",
            "range of games provides Maia-2 with access to more comprehensive and nuanced patterns in human\n",
            "chess moves. Using Maia-2 subset as a comparison, we can determine the relative contributions of\n",
            "model architecture and training data to Maia-2’s 1.9 percentage point gap over its nearest rival (Maia\n",
            "1500). This calculation suggests that 73% of the increase in performance is due to the architecture\n",
            "improvements and 27% is due to increased training data.\n",
            "7\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 4:\n",
            "• At submission time, to preserve anonymity, the authors should release anonymized\n",
            "versions (if applicable).\n",
            "• Providing as much information as possible in supplemental material (appended to the\n",
            "paper) is recommended, but including URLs to data and code is permitted.\n",
            "6. Experimental Setting/Details\n",
            "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-\n",
            "parameters, how they were chosen, type of optimizer, etc.) necessary to understand the\n",
            "results?\n",
            "Answer: [Yes]\n",
            "Justification: We include dataset details and hyperparameter settings in Section 4 and\n",
            "Appendix B.\n",
            "Guidelines:\n",
            "• The answer NA means that the paper does not include experiments.\n",
            "• The experimental setting should be presented in the core of the paper to a level of detail\n",
            "that is necessary to appreciate the results and make sense of them.\n",
            "• The full details can be provided either with the code, in appendix, or as supplemental\n",
            "material.\n",
            "7. Experiment Statistical Significance\n",
            "Question: Does the paper report error bars suitably and correctly defined or other appropriate\n",
            "information about the statistical significance of the experiments?\n",
            "Answer: [No]\n",
            "Justification: We train Maia-2 with a huge amount (9.1B) of chess positions. Therefore, it is\n",
            "hard to evaluate Maia-2 multiple times with different train/test splits.\n",
            "Guidelines:\n",
            "• The answer NA means that the paper does not include experiments.\n",
            "• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\n",
            "dence intervals, or statistical significance tests, at least for the experiments that support\n",
            "the main claims of the paper.\n",
            "• The factors of variability that the error bars are capturing should be clearly stated (for\n",
            "example, train/test split, initialization, random drawing of some parameter, or overall\n",
            "run with given experimental conditions).\n",
            "• The method for calculating the error bars should be explained (closed form formula,\n",
            "call to a library function, bootstrap, etc.)\n",
            "• The assumptions made should be given (e.g., Normally distributed errors).\n",
            "• It should be clear whether the error bar is the standard deviation or the standard error\n",
            "of the mean.\n",
            "• It is OK to report 1-sigma error bars, but one should state it. The authors should\n",
            "preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\n",
            "of Normality of errors is not verified.\n",
            "• For asymmetric distributions, the authors should be careful not to show in tables or\n",
            "figures symmetric error bars that would yield results that are out of range (e.g. negative\n",
            "error rates).\n",
            "• If error bars are reported in tables or plots, The authors should explain in the text how\n",
            "they were calculated and reference the corresponding figures or tables in the text.\n",
            "8. Experiments Compute Resources\n",
            "Question: For each experiment, does the paper provide sufficient information on the com-\n",
            "puter resources (type of compute workers, memory, time of execution) needed to reproduce\n",
            "the experiments?\n",
            "Answer: [Yes]\n",
            "Justification: We report the required computational resources in Appendix B.\n",
            "Guidelines:\n",
            "23\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 5:\n",
            "Prompt\n",
            "What determines which player starts first in a chess game? (Answer: white)\n",
            "ChessGPT-Base\n",
            "• White always moves first.\n",
            "• White moves first.\n",
            "• In chess, the player with the white pieces always moves first.\n",
            "ChessGPT-Chat\n",
            "• White and Black are both considered to have the advantage, and the player who is\n",
            "the first to move (the white player) is always the first player to move.\n",
            "• White is determined by the player with the light-colored pieces, also known as the\n",
            "\"white\" player.\n",
            "• In a game of chess, the player with the white pieces should start first.\n",
            "RedPajama\n",
            "• The first player to move is determined by the order in which the players are seated.\n",
            "The first player is seated to the left of the board.\n",
            "• The first player to move is determined by a sequence of moves called the \"opening\".\n",
            "This sequence of moves is determined by the player who moves first.\n",
            "• The first player to move is the one who moves first in the first game.\n",
            "Human Error Analysis\n",
            "• ChessGPT-Base: All answers clearly state that white always goes first, which is\n",
            "accurate.\n",
            "• ChessGPT-Chat: All answers mentioned that white or the player using white pawns\n",
            "should move first, which is also accurate. However, the \"both white and black are\n",
            "considered to have an advantage\" mentioned in the first answer is inaccurate,\n",
            "because usually white is considered to have a small opening advantage.\n",
            "• RedPajama: None of the answers explicitly state that white goes first, and the first\n",
            "and second answers are completely inaccurate. The third answer was vague and\n",
            "unclear.\n",
            "Table 20: Question on starting player.\n",
            "39\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/chessgpt.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the reranker model and tokenizer\n",
        "reranker_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
        "reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name)\n",
        "\n",
        "# Define a function to rerank documents\n",
        "def rerank_documents(query, documents, top_n=5):\n",
        "    # Return empty list if no documents are provided\n",
        "    if not documents:\n",
        "        return []\n",
        "\n",
        "    # Create pairs of query and document content for the cross-encoder\n",
        "    pairs = [[query, doc.page_content] for doc in documents]\n",
        "\n",
        "    # Use the reranker model to get scores for each pair\n",
        "    with torch.no_grad():\n",
        "        inputs = reranker_tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "        scores = reranker_model(**inputs).logits.squeeze(-1)\n",
        "\n",
        "    # Sort documents based on the reranker scores in descending order\n",
        "    sorted_docs = [documents[i] for i in scores.argsort(descending=True)]\n",
        "\n",
        "    # Print a message indicating reranking is done\n",
        "    print(f\"\\nReranked documents using {reranker_model_name}.\")\n",
        "\n",
        "    # Return the top_n reranked documents\n",
        "    return sorted_docs[:top_n]\n",
        "\n",
        "# Test the reranking function with RRF results\n",
        "reranked_rrf_docs = rerank_documents(sample_query, docs_rrf, top_n=5)\n",
        "\n",
        "# Print the top 5 reranked RRF documents\n",
        "print(f\"\\nTop 5 Reranked RRF documents:\")\n",
        "for i, doc in enumerate(reranked_rrf_docs):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content)\n",
        "    print(f\"Source: {doc.metadata.get('source')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TGmmiYuTKyk3",
        "outputId": "ffded877-787d-4e56-b3b6-5154f3fe53a1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reranked documents using cross-encoder/ms-marco-MiniLM-L-6-v2.\n",
            "\n",
            "Top 5 Reranked RRF documents:\n",
            "\n",
            "Document 1:\n",
            "Figure 4: Maia-2’s chess concept recognition as a function of skill level, as measured by linear\n",
            "activation probes right before (blue) and after (orange) skill-aware attention. (a) Stockfish overall\n",
            "board evaluation for middle-game positions. (b) Stockfish evaluation of middle-game bonuses and\n",
            "penalties to pieces for white. (c) Does the active player own two bishops? (d) Can the active player\n",
            "capture the opponent’s queen?\n",
            "subtle nuances. We now turn our focus to a critical question: does Maia-2 vary in its ability to capture\n",
            "human chess concepts when given different skill levels? Following the chess concepts probing\n",
            "strategy for AlphaZero [22], we show how Maia-2’s grasp of various concepts varies with skill. The\n",
            "left two plots in Figure 4 show concepts for which Maia-2 clearly distinguishes between skill levels,\n",
            "with higher-skill players paying more attention to them than lower-skill players. These are general\n",
            "board evaluations as given by Stockfish [ 6], or aggregate piece values. Note that pre-skill-aware\n",
            "attention is always flat because by construction it cannot vary with skill, since skill-aware attention has\n",
            "not been applied yet. The two plots on the right depict concepts that live closer to fundamental chess\n",
            "rules, and as such are less dependent on player skill. For skill-dependent concepts, the figures reveal\n",
            "an increasing trend in mastery level after skill-aware attention, aligning with the increase in dedicated\n",
            "skill levels. Meanwhile, the model’s mastery level decreases after passing through the skill-aware\n",
            "attention modules, potentially adjusting for the imperfections of human players. Conversely, the\n",
            "skill-aware attention blocks are not responsive to skill-independent concepts.\n",
            "5 Discussion\n",
            "Human Study. In addition to human move matching, we also consider engagement, another\n",
            "dimension of human study. In particular, we implement a randomized experiment on Lichess:\n",
            "human players challenge our bots, and we randomize whether players play against Maia-1 or Maia-2.\n",
            "Our result is that our higher move-matching and our vastly improved coherence, across all skill\n",
            "levels, come at no cost to human subject engagement, and in fact slightly increase engagement:\n",
            "players rematch Maia-2 almost 1 percentage point more than Maia-1 (41.2% vs. 40.3%). Although\n",
            "engagement is not our main objective, this is further promising evidence that we have achieved a\n",
            "more human-aligned model that coherently captures human style across different skill levels.\n",
            "Ethical Considerations. We believe Maia-2 poses limited risk while offering large potential benefits.\n",
            "Our data is highly aggregated, with almost 1 billion games being used for training, and chess as a\n",
            "domain is generally low-risk. Meanwhile, helping people improve in chess could lead to increased\n",
            "cognitive skills, confidence boosts, and help with general life satisfaction. Our vision is for Maia-2 to\n",
            "power AI partners and training aids; it cannot currently replace skilled human tutors and coaches.\n",
            "Limitation. Our work has limitations. First, we are excited by the applications that Maia-2 will\n",
            "enable, such as more relatable AI partners and AI-powered learning aids, the development of which is\n",
            "out of scope for the current work. Maia-2 does not yet incorporate search, although previous work has\n",
            "demonstrated that with proper regularization it can help improve move prediction performance [26].\n",
            "Relatedly, we group the strongest players in a single bucket, although modeling the very best players\n",
            "in the world remains difficult due to the complexity and depth of their moves.\n",
            "10\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 2:\n",
            "interact with chess positions to produce the moves humans make. Unlike previous models, Maia-2\n",
            "only requires the current board position as input (as opposed to six), which dramatically reduces\n",
            "training time and increases flexibility (e.g. for applying the model in non-game contexts where there\n",
            "may be no 6-board history). In addition to policy and value heads like in previous work, we also add\n",
            "an additional auxiliary information head that helps the model learn a deeper understanding of human\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 3:\n",
            "• At submission time, to preserve anonymity, the authors should release anonymized\n",
            "versions (if applicable).\n",
            "• Providing as much information as possible in supplemental material (appended to the\n",
            "paper) is recommended, but including URLs to data and code is permitted.\n",
            "6. Experimental Setting/Details\n",
            "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-\n",
            "parameters, how they were chosen, type of optimizer, etc.) necessary to understand the\n",
            "results?\n",
            "Answer: [Yes]\n",
            "Justification: We include dataset details and hyperparameter settings in Section 4 and\n",
            "Appendix B.\n",
            "Guidelines:\n",
            "• The answer NA means that the paper does not include experiments.\n",
            "• The experimental setting should be presented in the core of the paper to a level of detail\n",
            "that is necessary to appreciate the results and make sense of them.\n",
            "• The full details can be provided either with the code, in appendix, or as supplemental\n",
            "material.\n",
            "7. Experiment Statistical Significance\n",
            "Question: Does the paper report error bars suitably and correctly defined or other appropriate\n",
            "information about the statistical significance of the experiments?\n",
            "Answer: [No]\n",
            "Justification: We train Maia-2 with a huge amount (9.1B) of chess positions. Therefore, it is\n",
            "hard to evaluate Maia-2 multiple times with different train/test splits.\n",
            "Guidelines:\n",
            "• The answer NA means that the paper does not include experiments.\n",
            "• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\n",
            "dence intervals, or statistical significance tests, at least for the experiments that support\n",
            "the main claims of the paper.\n",
            "• The factors of variability that the error bars are capturing should be clearly stated (for\n",
            "example, train/test split, initialization, random drawing of some parameter, or overall\n",
            "run with given experimental conditions).\n",
            "• The method for calculating the error bars should be explained (closed form formula,\n",
            "call to a library function, bootstrap, etc.)\n",
            "• The assumptions made should be given (e.g., Normally distributed errors).\n",
            "• It should be clear whether the error bar is the standard deviation or the standard error\n",
            "of the mean.\n",
            "• It is OK to report 1-sigma error bars, but one should state it. The authors should\n",
            "preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\n",
            "of Normality of errors is not verified.\n",
            "• For asymmetric distributions, the authors should be careful not to show in tables or\n",
            "figures symmetric error bars that would yield results that are out of range (e.g. negative\n",
            "error rates).\n",
            "• If error bars are reported in tables or plots, The authors should explain in the text how\n",
            "they were calculated and reference the corresponding figures or tables in the text.\n",
            "8. Experiments Compute Resources\n",
            "Question: For each experiment, does the paper provide sufficient information on the com-\n",
            "puter resources (type of compute workers, memory, time of execution) needed to reproduce\n",
            "the experiments?\n",
            "Answer: [Yes]\n",
            "Justification: We report the required computational resources in Appendix B.\n",
            "Guidelines:\n",
            "23\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 4:\n",
            "1200 1600 2000\n",
            "Active Player's Skill Level\n",
            "1200 1600 2000\n",
            "Opponent Player's Skill Level\n",
            "48.0 47.5 47.3 46.1\n",
            "49.2 48.4 48.8 48.1 47.5\n",
            "49.5 49.3 49.0 49.1 48.4 47.8\n",
            "50.1 50.0 49.9 49.8 49.2 48.7 48.6\n",
            "50.4 50.8 50.6 50.4 50.1 50.1 49.6\n",
            "51.0 50.9 51.5 50.9 51.0 50.7 50.8\n",
            "50.2 50.5 51.5 51.6 51.5 51.5 51.4\n",
            "50.5 51.9 51.6 52.0 50.8 51.8\n",
            "51.5 51.9 52.2 52.2 50.6\n",
            "51.6 51.8 52.0 52.8\n",
            "Maia 1100\n",
            "1200 1600 2000\n",
            "Active Player's Skill Level\n",
            "51.1 52.0 52.5 52.9\n",
            "51.1 51.5 52.8 52.9 53.3\n",
            "49.4 51.2 51.6 52.9 53.0 54.0\n",
            "49.0 49.9 51.4 52.0 52.7 53.5 54.4\n",
            "47.8 49.5 50.9 51.6 52.2 53.3 54.2\n",
            "47.5 48.3 50.4 50.5 51.8 52.9 53.9\n",
            "46.2 47.3 49.6 50.3 50.7 52.0 53.0\n",
            "46.3 48.7 49.2 50.3 49.8 52.3\n",
            "47.7 48.4 49.4 50.2 49.0\n",
            "47.3 48.3 48.8 50.2\n",
            "Maia 1900\n",
            "1200 1600 2000\n",
            "Active Player's Skill Level\n",
            "52.4 53.1 53.8 53.6\n",
            "52.7 53.3 54.2 54.2 54.3\n",
            "51.5 53.0 53.2 54.3 54.5 55.0\n",
            "51.6 52.2 53.4 53.8 54.4 54.9 55.7\n",
            "51.1 52.1 53.1 53.7 54.2 55.2 55.6\n",
            "51.1 51.5 53.4 53.1 54.0 54.8 55.6\n",
            "50.0 50.6 52.8 53.4 53.3 54.5 55.1\n",
            "50.2 52.2 52.6 53.4 52.3 54.8\n",
            "51.4 52.0 52.9 53.3 52.2\n",
            "51.2 52.1 52.4 53.9\n",
            "Maia-2\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "Move Prediction Accuracy\n",
            "Figure 2: Move prediction accuracy across diverse skill levels. Colors represent performance, with\n",
            "warmer tones indicating higher accuracy.\n",
            "4.1 Move Prediction Accuracy\n",
            "Maia-2. In Table 1, we show the top-1 move prediction accuracy of all models across all groups of\n",
            "players on the Maia-1 Testset. Maia-2 demonstrates strong and consistent performance across all\n",
            "skill levels, surpassing all baselines. Specifically, despite Maia-1 models being specifically trained to\n",
            "mimic chess moves by players at specific skill levels, Maia-2 emerges as a unified one-for-all model\n",
            "that is consistently effective across the entire spectrum of chess skills. The largest improvement is\n",
            "on Advanced players, where Maia-2 gains 1.5 percentage points over the nearest competitor (Maia\n",
            "1500). When averaging across skill levels, Maia-2 outperforms all other models by almost 2 full\n",
            "percentage points in overall accuracy. Note that the ceiling accuracy of human move prediction is far\n",
            "below 100% given the randomness and diversity of human decisions—even thesame player won’t\n",
            "always make the same decision when faced with the same position. Our 2 percentage point gain is\n",
            "substantial considering that the difference between Maia-1 and Leela, the previous state-of-the-art\n",
            "model for this task and a traditional chess engine not trained for this task at all, is only 6 percentage\n",
            "points. Furthermore, Maia-1 is essentially a mixture of 9 experts targeting the specific players’\n",
            "skill level, where each expert has 10.3M parameters. Regarding the routing function to select the\n",
            "best-performing expert as a nonparameterized function, Maia-1 has 92M parameters in total. Maia-2,\n",
            "on the other hand, is a one-for-all model with 23.3M parameters under our default settings. Therefore,\n",
            "Maia-2 achieves better human move prediction accuracy with even much fewer trainable parameters.\n",
            "Baseline models. Both Maia-2 and Maia-1 significantly outperform Stockfish and Leela, typically by\n",
            "5–15 percentage points. Note that Stockfish and Leela aim to play optimal chess (as most humans do\n",
            "too), and only “predict” human moves when their approximations to optimality happen to overlap\n",
            "with those of human players. However, we compare to these traditional chess engines because besides\n",
            "Maia-1, there are still the default method of creating “human-like” AI agents. The accuracy gap\n",
            "between Maia-1 architectures and traditional chess engines demonstrates the necessity of developing\n",
            "specialized models to mimic human chess moves.\n",
            "Maia-2subset. Maia-2 differs from Maia-1 in two main ways: it has a different architecture and it\n",
            "has access to more training data. To control for the difference in training data and isolate the effects\n",
            "of our architecture, we create Maia-2 subset which has access to the exact same training data that\n",
            "Maia-1 was developed with. Comparing the two, we see that Maia-2subset matches or outperforms\n",
            "all baselines and alternate models. Recall that Maia-2 and Maia-2subset don’t have the recent history\n",
            "passed as input to them, yet still achieve state-of-the-art results. It is important to note that each\n",
            "Maia-1 model is specifically trained for its respective skill level, relying solely on games where the\n",
            "active and opponent skill levels match for its training data. On the contrary, the unified modeling\n",
            "approach with skill-aware attention of Maia-2subset allows it to utilize a broader spectrum of games,\n",
            "featuring a variety of skill-level pairings, for training purposes. Consequently, while both Maia-1 and\n",
            "Maia-2subset draw from the same source dataset, Maia-2subset can leverage a significantly larger portion\n",
            "of this data for its training, improving its learning and predictive capabilities. The improvement from\n",
            "Maia-2subset to Maia-2 underscores the importance of extensive training with vast datasets. A broader\n",
            "range of games provides Maia-2 with access to more comprehensive and nuanced patterns in human\n",
            "chess moves. Using Maia-2 subset as a comparison, we can determine the relative contributions of\n",
            "model architecture and training data to Maia-2’s 1.9 percentage point gap over its nearest rival (Maia\n",
            "1500). This calculation suggests that 73% of the increase in performance is due to the architecture\n",
            "improvements and 27% is due to increased training data.\n",
            "7\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 5:\n",
            "Prompt\n",
            "What determines which player starts first in a chess game? (Answer: white)\n",
            "ChessGPT-Base\n",
            "• White always moves first.\n",
            "• White moves first.\n",
            "• In chess, the player with the white pieces always moves first.\n",
            "ChessGPT-Chat\n",
            "• White and Black are both considered to have the advantage, and the player who is\n",
            "the first to move (the white player) is always the first player to move.\n",
            "• White is determined by the player with the light-colored pieces, also known as the\n",
            "\"white\" player.\n",
            "• In a game of chess, the player with the white pieces should start first.\n",
            "RedPajama\n",
            "• The first player to move is determined by the order in which the players are seated.\n",
            "The first player is seated to the left of the board.\n",
            "• The first player to move is determined by a sequence of moves called the \"opening\".\n",
            "This sequence of moves is determined by the player who moves first.\n",
            "• The first player to move is the one who moves first in the first game.\n",
            "Human Error Analysis\n",
            "• ChessGPT-Base: All answers clearly state that white always goes first, which is\n",
            "accurate.\n",
            "• ChessGPT-Chat: All answers mentioned that white or the player using white pawns\n",
            "should move first, which is also accurate. However, the \"both white and black are\n",
            "considered to have an advantage\" mentioned in the first answer is inaccurate,\n",
            "because usually white is considered to have a small opening advantage.\n",
            "• RedPajama: None of the answers explicitly state that white goes first, and the first\n",
            "and second answers are completely inaccurate. The third answer was vague and\n",
            "unclear.\n",
            "Table 20: Question on starting player.\n",
            "39\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/chessgpt.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the EmbeddingsRedundantFilter for compression\n",
        "redundant_filter = EmbeddingsRedundantFilter(embeddings=embedding_function)\n",
        "\n",
        "# Create a DocumentCompressorPipeline with the redundant filter\n",
        "compression_pipeline = DocumentCompressorPipeline(transformers=[redundant_filter])\n",
        "\n",
        "print(\"\\nCompression pipeline initialized with EmbeddingsRedundantFilter.\")\n",
        "\n",
        "# Define a function to retrieve, rerank, and compress documents\n",
        "def retrieve_and_compress(query, rrf_retriever, reranker_function, compressor_pipeline, top_n_rerank=5):\n",
        "    # Perform RRF retrieval\n",
        "    print(f\"\\nPerforming RRF retrieval for query: {query}\")\n",
        "    initial_docs = rrf_retriever.invoke(query)\n",
        "\n",
        "    # Rerank the retrieved documents\n",
        "    print(f\"Reranking {len(initial_docs)} retrieved documents.\")\n",
        "    reranked_docs = reranker_function(query, initial_docs, top_n=top_n_rerank)\n",
        "\n",
        "    # Apply compression to the reranked documents\n",
        "    print(f\"Applying compression to {len(reranked_docs)} reranked documents.\")\n",
        "    compressed_docs = compressor_pipeline.compress_documents(reranked_docs, query=query)\n",
        "\n",
        "    return compressed_docs\n",
        "\n",
        "# Test the combined retrieval and compression process\n",
        "final_retrieved_compressed_docs = retrieve_and_compress(\n",
        "    sample_query, # Use the predefined sample_query\n",
        "    ensemble_retriever, # Use the RRF retriever\n",
        "    rerank_documents, # Use the reranking function\n",
        "    compression_pipeline, # Use the compression pipeline\n",
        "    top_n_rerank=5 # Specify the number of documents to keep after reranking\n",
        ")\n",
        "\n",
        "# Print the final retrieved and compressed documents\n",
        "print(f\"\\nFinal Retrieved and Compressed Documents (RRF -> Reranking -> Compression):\")\n",
        "for i, doc in enumerate(final_retrieved_compressed_docs):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content)\n",
        "    print(f\"Source: {doc.metadata.get('source')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3mTr1tDLH2L",
        "outputId": "cff0f9c4-7783-44e1-b16b-75b9ddda14be"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Compression pipeline initialized with EmbeddingsRedundantFilter.\n",
            "\n",
            "Performing RRF retrieval for query: What is the main idea of the Maia-2 paper?\n",
            "Reranking 5 retrieved documents.\n",
            "\n",
            "Reranked documents using cross-encoder/ms-marco-MiniLM-L-6-v2.\n",
            "Applying compression to 5 reranked documents.\n",
            "\n",
            "Final Retrieved and Compressed Documents (RRF -> Reranking -> Compression):\n",
            "\n",
            "Document 1:\n",
            "Figure 4: Maia-2’s chess concept recognition as a function of skill level, as measured by linear\n",
            "activation probes right before (blue) and after (orange) skill-aware attention. (a) Stockfish overall\n",
            "board evaluation for middle-game positions. (b) Stockfish evaluation of middle-game bonuses and\n",
            "penalties to pieces for white. (c) Does the active player own two bishops? (d) Can the active player\n",
            "capture the opponent’s queen?\n",
            "subtle nuances. We now turn our focus to a critical question: does Maia-2 vary in its ability to capture\n",
            "human chess concepts when given different skill levels? Following the chess concepts probing\n",
            "strategy for AlphaZero [22], we show how Maia-2’s grasp of various concepts varies with skill. The\n",
            "left two plots in Figure 4 show concepts for which Maia-2 clearly distinguishes between skill levels,\n",
            "with higher-skill players paying more attention to them than lower-skill players. These are general\n",
            "board evaluations as given by Stockfish [ 6], or aggregate piece values. Note that pre-skill-aware\n",
            "attention is always flat because by construction it cannot vary with skill, since skill-aware attention has\n",
            "not been applied yet. The two plots on the right depict concepts that live closer to fundamental chess\n",
            "rules, and as such are less dependent on player skill. For skill-dependent concepts, the figures reveal\n",
            "an increasing trend in mastery level after skill-aware attention, aligning with the increase in dedicated\n",
            "skill levels. Meanwhile, the model’s mastery level decreases after passing through the skill-aware\n",
            "attention modules, potentially adjusting for the imperfections of human players. Conversely, the\n",
            "skill-aware attention blocks are not responsive to skill-independent concepts.\n",
            "5 Discussion\n",
            "Human Study. In addition to human move matching, we also consider engagement, another\n",
            "dimension of human study. In particular, we implement a randomized experiment on Lichess:\n",
            "human players challenge our bots, and we randomize whether players play against Maia-1 or Maia-2.\n",
            "Our result is that our higher move-matching and our vastly improved coherence, across all skill\n",
            "levels, come at no cost to human subject engagement, and in fact slightly increase engagement:\n",
            "players rematch Maia-2 almost 1 percentage point more than Maia-1 (41.2% vs. 40.3%). Although\n",
            "engagement is not our main objective, this is further promising evidence that we have achieved a\n",
            "more human-aligned model that coherently captures human style across different skill levels.\n",
            "Ethical Considerations. We believe Maia-2 poses limited risk while offering large potential benefits.\n",
            "Our data is highly aggregated, with almost 1 billion games being used for training, and chess as a\n",
            "domain is generally low-risk. Meanwhile, helping people improve in chess could lead to increased\n",
            "cognitive skills, confidence boosts, and help with general life satisfaction. Our vision is for Maia-2 to\n",
            "power AI partners and training aids; it cannot currently replace skilled human tutors and coaches.\n",
            "Limitation. Our work has limitations. First, we are excited by the applications that Maia-2 will\n",
            "enable, such as more relatable AI partners and AI-powered learning aids, the development of which is\n",
            "out of scope for the current work. Maia-2 does not yet incorporate search, although previous work has\n",
            "demonstrated that with proper regularization it can help improve move prediction performance [26].\n",
            "Relatedly, we group the strongest players in a single bucket, although modeling the very best players\n",
            "in the world remains difficult due to the complexity and depth of their moves.\n",
            "10\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 2:\n",
            "interact with chess positions to produce the moves humans make. Unlike previous models, Maia-2\n",
            "only requires the current board position as input (as opposed to six), which dramatically reduces\n",
            "training time and increases flexibility (e.g. for applying the model in non-game contexts where there\n",
            "may be no 6-board history). In addition to policy and value heads like in previous work, we also add\n",
            "an additional auxiliary information head that helps the model learn a deeper understanding of human\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 3:\n",
            "• At submission time, to preserve anonymity, the authors should release anonymized\n",
            "versions (if applicable).\n",
            "• Providing as much information as possible in supplemental material (appended to the\n",
            "paper) is recommended, but including URLs to data and code is permitted.\n",
            "6. Experimental Setting/Details\n",
            "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-\n",
            "parameters, how they were chosen, type of optimizer, etc.) necessary to understand the\n",
            "results?\n",
            "Answer: [Yes]\n",
            "Justification: We include dataset details and hyperparameter settings in Section 4 and\n",
            "Appendix B.\n",
            "Guidelines:\n",
            "• The answer NA means that the paper does not include experiments.\n",
            "• The experimental setting should be presented in the core of the paper to a level of detail\n",
            "that is necessary to appreciate the results and make sense of them.\n",
            "• The full details can be provided either with the code, in appendix, or as supplemental\n",
            "material.\n",
            "7. Experiment Statistical Significance\n",
            "Question: Does the paper report error bars suitably and correctly defined or other appropriate\n",
            "information about the statistical significance of the experiments?\n",
            "Answer: [No]\n",
            "Justification: We train Maia-2 with a huge amount (9.1B) of chess positions. Therefore, it is\n",
            "hard to evaluate Maia-2 multiple times with different train/test splits.\n",
            "Guidelines:\n",
            "• The answer NA means that the paper does not include experiments.\n",
            "• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\n",
            "dence intervals, or statistical significance tests, at least for the experiments that support\n",
            "the main claims of the paper.\n",
            "• The factors of variability that the error bars are capturing should be clearly stated (for\n",
            "example, train/test split, initialization, random drawing of some parameter, or overall\n",
            "run with given experimental conditions).\n",
            "• The method for calculating the error bars should be explained (closed form formula,\n",
            "call to a library function, bootstrap, etc.)\n",
            "• The assumptions made should be given (e.g., Normally distributed errors).\n",
            "• It should be clear whether the error bar is the standard deviation or the standard error\n",
            "of the mean.\n",
            "• It is OK to report 1-sigma error bars, but one should state it. The authors should\n",
            "preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\n",
            "of Normality of errors is not verified.\n",
            "• For asymmetric distributions, the authors should be careful not to show in tables or\n",
            "figures symmetric error bars that would yield results that are out of range (e.g. negative\n",
            "error rates).\n",
            "• If error bars are reported in tables or plots, The authors should explain in the text how\n",
            "they were calculated and reference the corresponding figures or tables in the text.\n",
            "8. Experiments Compute Resources\n",
            "Question: For each experiment, does the paper provide sufficient information on the com-\n",
            "puter resources (type of compute workers, memory, time of execution) needed to reproduce\n",
            "the experiments?\n",
            "Answer: [Yes]\n",
            "Justification: We report the required computational resources in Appendix B.\n",
            "Guidelines:\n",
            "23\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 4:\n",
            "1200 1600 2000\n",
            "Active Player's Skill Level\n",
            "1200 1600 2000\n",
            "Opponent Player's Skill Level\n",
            "48.0 47.5 47.3 46.1\n",
            "49.2 48.4 48.8 48.1 47.5\n",
            "49.5 49.3 49.0 49.1 48.4 47.8\n",
            "50.1 50.0 49.9 49.8 49.2 48.7 48.6\n",
            "50.4 50.8 50.6 50.4 50.1 50.1 49.6\n",
            "51.0 50.9 51.5 50.9 51.0 50.7 50.8\n",
            "50.2 50.5 51.5 51.6 51.5 51.5 51.4\n",
            "50.5 51.9 51.6 52.0 50.8 51.8\n",
            "51.5 51.9 52.2 52.2 50.6\n",
            "51.6 51.8 52.0 52.8\n",
            "Maia 1100\n",
            "1200 1600 2000\n",
            "Active Player's Skill Level\n",
            "51.1 52.0 52.5 52.9\n",
            "51.1 51.5 52.8 52.9 53.3\n",
            "49.4 51.2 51.6 52.9 53.0 54.0\n",
            "49.0 49.9 51.4 52.0 52.7 53.5 54.4\n",
            "47.8 49.5 50.9 51.6 52.2 53.3 54.2\n",
            "47.5 48.3 50.4 50.5 51.8 52.9 53.9\n",
            "46.2 47.3 49.6 50.3 50.7 52.0 53.0\n",
            "46.3 48.7 49.2 50.3 49.8 52.3\n",
            "47.7 48.4 49.4 50.2 49.0\n",
            "47.3 48.3 48.8 50.2\n",
            "Maia 1900\n",
            "1200 1600 2000\n",
            "Active Player's Skill Level\n",
            "52.4 53.1 53.8 53.6\n",
            "52.7 53.3 54.2 54.2 54.3\n",
            "51.5 53.0 53.2 54.3 54.5 55.0\n",
            "51.6 52.2 53.4 53.8 54.4 54.9 55.7\n",
            "51.1 52.1 53.1 53.7 54.2 55.2 55.6\n",
            "51.1 51.5 53.4 53.1 54.0 54.8 55.6\n",
            "50.0 50.6 52.8 53.4 53.3 54.5 55.1\n",
            "50.2 52.2 52.6 53.4 52.3 54.8\n",
            "51.4 52.0 52.9 53.3 52.2\n",
            "51.2 52.1 52.4 53.9\n",
            "Maia-2\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "Move Prediction Accuracy\n",
            "Figure 2: Move prediction accuracy across diverse skill levels. Colors represent performance, with\n",
            "warmer tones indicating higher accuracy.\n",
            "4.1 Move Prediction Accuracy\n",
            "Maia-2. In Table 1, we show the top-1 move prediction accuracy of all models across all groups of\n",
            "players on the Maia-1 Testset. Maia-2 demonstrates strong and consistent performance across all\n",
            "skill levels, surpassing all baselines. Specifically, despite Maia-1 models being specifically trained to\n",
            "mimic chess moves by players at specific skill levels, Maia-2 emerges as a unified one-for-all model\n",
            "that is consistently effective across the entire spectrum of chess skills. The largest improvement is\n",
            "on Advanced players, where Maia-2 gains 1.5 percentage points over the nearest competitor (Maia\n",
            "1500). When averaging across skill levels, Maia-2 outperforms all other models by almost 2 full\n",
            "percentage points in overall accuracy. Note that the ceiling accuracy of human move prediction is far\n",
            "below 100% given the randomness and diversity of human decisions—even thesame player won’t\n",
            "always make the same decision when faced with the same position. Our 2 percentage point gain is\n",
            "substantial considering that the difference between Maia-1 and Leela, the previous state-of-the-art\n",
            "model for this task and a traditional chess engine not trained for this task at all, is only 6 percentage\n",
            "points. Furthermore, Maia-1 is essentially a mixture of 9 experts targeting the specific players’\n",
            "skill level, where each expert has 10.3M parameters. Regarding the routing function to select the\n",
            "best-performing expert as a nonparameterized function, Maia-1 has 92M parameters in total. Maia-2,\n",
            "on the other hand, is a one-for-all model with 23.3M parameters under our default settings. Therefore,\n",
            "Maia-2 achieves better human move prediction accuracy with even much fewer trainable parameters.\n",
            "Baseline models. Both Maia-2 and Maia-1 significantly outperform Stockfish and Leela, typically by\n",
            "5–15 percentage points. Note that Stockfish and Leela aim to play optimal chess (as most humans do\n",
            "too), and only “predict” human moves when their approximations to optimality happen to overlap\n",
            "with those of human players. However, we compare to these traditional chess engines because besides\n",
            "Maia-1, there are still the default method of creating “human-like” AI agents. The accuracy gap\n",
            "between Maia-1 architectures and traditional chess engines demonstrates the necessity of developing\n",
            "specialized models to mimic human chess moves.\n",
            "Maia-2subset. Maia-2 differs from Maia-1 in two main ways: it has a different architecture and it\n",
            "has access to more training data. To control for the difference in training data and isolate the effects\n",
            "of our architecture, we create Maia-2 subset which has access to the exact same training data that\n",
            "Maia-1 was developed with. Comparing the two, we see that Maia-2subset matches or outperforms\n",
            "all baselines and alternate models. Recall that Maia-2 and Maia-2subset don’t have the recent history\n",
            "passed as input to them, yet still achieve state-of-the-art results. It is important to note that each\n",
            "Maia-1 model is specifically trained for its respective skill level, relying solely on games where the\n",
            "active and opponent skill levels match for its training data. On the contrary, the unified modeling\n",
            "approach with skill-aware attention of Maia-2subset allows it to utilize a broader spectrum of games,\n",
            "featuring a variety of skill-level pairings, for training purposes. Consequently, while both Maia-1 and\n",
            "Maia-2subset draw from the same source dataset, Maia-2subset can leverage a significantly larger portion\n",
            "of this data for its training, improving its learning and predictive capabilities. The improvement from\n",
            "Maia-2subset to Maia-2 underscores the importance of extensive training with vast datasets. A broader\n",
            "range of games provides Maia-2 with access to more comprehensive and nuanced patterns in human\n",
            "chess moves. Using Maia-2 subset as a comparison, we can determine the relative contributions of\n",
            "model architecture and training data to Maia-2’s 1.9 percentage point gap over its nearest rival (Maia\n",
            "1500). This calculation suggests that 73% of the increase in performance is due to the architecture\n",
            "improvements and 27% is due to increased training data.\n",
            "7\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 5:\n",
            "Prompt\n",
            "What determines which player starts first in a chess game? (Answer: white)\n",
            "ChessGPT-Base\n",
            "• White always moves first.\n",
            "• White moves first.\n",
            "• In chess, the player with the white pieces always moves first.\n",
            "ChessGPT-Chat\n",
            "• White and Black are both considered to have the advantage, and the player who is\n",
            "the first to move (the white player) is always the first player to move.\n",
            "• White is determined by the player with the light-colored pieces, also known as the\n",
            "\"white\" player.\n",
            "• In a game of chess, the player with the white pieces should start first.\n",
            "RedPajama\n",
            "• The first player to move is determined by the order in which the players are seated.\n",
            "The first player is seated to the left of the board.\n",
            "• The first player to move is determined by a sequence of moves called the \"opening\".\n",
            "This sequence of moves is determined by the player who moves first.\n",
            "• The first player to move is the one who moves first in the first game.\n",
            "Human Error Analysis\n",
            "• ChessGPT-Base: All answers clearly state that white always goes first, which is\n",
            "accurate.\n",
            "• ChessGPT-Chat: All answers mentioned that white or the player using white pawns\n",
            "should move first, which is also accurate. However, the \"both white and black are\n",
            "considered to have an advantage\" mentioned in the first answer is inaccurate,\n",
            "because usually white is considered to have a small opening advantage.\n",
            "• RedPajama: None of the answers explicitly state that white goes first, and the first\n",
            "and second answers are completely inaccurate. The third answer was vague and\n",
            "unclear.\n",
            "Table 20: Question on starting player.\n",
            "39\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/chessgpt.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the output directory\n",
        "output_dir = '/content/drive/MyDrive/Capstone/Week 5_Advanced_RAG'\n",
        "log_file_path = os.path.join(output_dir, \"retrieval_metrics_log.json\")\n",
        "\n",
        "# Function to measure latency and average context length\n",
        "def measure_retrieval_metrics(retriever_function, query):\n",
        "    start_time = time.time()\n",
        "    retrieved_docs = retriever_function(query) # Execute the retrieval process\n",
        "    end_time = time.time()\n",
        "    latency = end_time - start_time\n",
        "\n",
        "    # Calculate average context length\n",
        "    total_length = sum(len(doc.page_content) for doc in retrieved_docs)\n",
        "    avg_context_length = total_length / len(retrieved_docs) if retrieved_docs else 0\n",
        "\n",
        "    return latency, avg_context_length, retrieved_docs\n",
        "\n",
        "# Measure metrics for the RRF + Rerank + Compression pipeline\n",
        "# We will use the retrieve_and_compress function defined previously\n",
        "# Make sure ensemble_retriever, rerank_documents, and compression_pipeline are defined and accessible\n",
        "try:\n",
        "    latency, avg_context_length, retrieved_docs = measure_retrieval_metrics(\n",
        "        lambda q: retrieve_and_compress(q, ensemble_retriever, rerank_documents, compression_pipeline, top_n_rerank=5),\n",
        "        sample_query # Use the sample query for testing\n",
        "    )\n",
        "\n",
        "    # Prepare the metrics to log\n",
        "    metrics = {\n",
        "        \"retriever\": \"RRF + Rerank + Compression\",\n",
        "        \"query\": sample_query,\n",
        "        \"latency_seconds\": latency,\n",
        "        \"average_context_length\": avg_context_length,\n",
        "        \"num_retrieved_docs\": len(retrieved_docs)\n",
        "    }\n",
        "\n",
        "    # Check if the log file exists and load existing data\n",
        "    existing_logs = []\n",
        "    if os.path.exists(log_file_path):\n",
        "        try:\n",
        "            with open(log_file_path, 'r') as f:\n",
        "                existing_logs = json.load(f)\n",
        "        except json.JSONDecodeError:\n",
        "            existing_logs = [] # Handle empty or invalid JSON\n",
        "\n",
        "    # Append new metrics\n",
        "    existing_logs.append(metrics)\n",
        "\n",
        "    # Save the updated logs to the file\n",
        "    with open(log_file_path, 'w') as f:\n",
        "        json.dump(existing_logs, f, indent=4)\n",
        "\n",
        "    print(f\"\\nLogged retrieval metrics to {log_file_path}\")\n",
        "    print(json.dumps(metrics, indent=4))\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"\\nError: Required variables or functions are not defined. Please ensure ensemble_retriever, rerank_documents, and compression_pipeline are executed before this cell.\")\n",
        "    print(e)\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn unexpected error occurred during metric measurement: {e}\")\n",
        "\n",
        "\n",
        "# Note on Recall@k and Token Cost:\n",
        "# Recall@k requires an evaluation dataset with ground truth relevant documents for each query.\n",
        "# Token cost is relevant if an LLM is used in the pipeline (e.g., for compression/summarization).\n",
        "# To implement these, you would need to:\n",
        "# 1. Create or load an evaluation dataset.\n",
        "# 2. For Recall@k, compare the retrieved documents against the ground truth relevant documents for each query in the dataset.\n",
        "# 3. For Token cost (with LLM), track the token usage of the LLM during the compression step."
      ],
      "metadata": {
        "id": "9xpubacPl3kY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97e9ab8f-ed4f-497d-f473-fa5651654532"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performing RRF retrieval for query: What is the main idea of the Maia-2 paper?\n",
            "Reranking 5 retrieved documents.\n",
            "\n",
            "Reranked documents using cross-encoder/ms-marco-MiniLM-L-6-v2.\n",
            "Applying compression to 5 reranked documents.\n",
            "\n",
            "Logged retrieval metrics to /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/retrieval_metrics_log.json\n",
            "{\n",
            "    \"retriever\": \"RRF + Rerank + Compression\",\n",
            "    \"query\": \"What is the main idea of the Maia-2 paper?\",\n",
            "    \"latency_seconds\": 1.2836945056915283,\n",
            "    \"average_context_length\": 2817.0,\n",
            "    \"num_retrieved_docs\": 5\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a913682"
      },
      "source": [
        "## Step 4 (Track B)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Multimodal Retrieval (Track B)\n",
        "\n",
        "# Add image/chart support via captions/embeddings (e.g., Gemini-Vision, BLIP2)\n",
        "# Build a joint index for text + image\n",
        "# Demonstrate text-only, image-only, and hybrid queries with grounded citations\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configure the Gemini API\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API')\n",
        "    if not GOOGLE_API_KEY:\n",
        "        raise ValueError(\"Google API Key not found in Colab secrets.\")\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\"Google Generative AI configured successfully.\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"Google API Key not found in Colab secrets.\")\n",
        "    print(\"Please add your GOOGLE_API_KEY to Colab secrets.\")\n",
        "except ValueError as e:\n",
        "    print(e)\n",
        "    print(\"Please ensure your GOOGLE_API_KEY is correctly set up.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during Gemini API configuration: {e}\")\n",
        "    genai = None # Set genai to None if configuration fails\n",
        "\n",
        "\n",
        "# Function to generate captions for images using Gemini-2.5-pro\n",
        "def generate_image_caption(image_path, prompt=\"Describe this image in detail.\"):\n",
        "    \"\"\"\n",
        "    Generates a caption for an image using a Gemini-Vision model.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): The path to the image file.\n",
        "        prompt (str): The prompt to guide the caption generation.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated caption, or None if an error occurred.\n",
        "    \"\"\"\n",
        "    if not genai:\n",
        "        print(\"Gemini API not configured. Cannot generate caption.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Load the Gemini-2.5-pro model\n",
        "        multimodal_model = genai.GenerativeModel('gemini-2.5-pro')\n",
        "\n",
        "        # Read the image file\n",
        "        import PIL.Image\n",
        "        img = PIL.Image.open(image_path)\n",
        "\n",
        "        # Generate content (caption) based on the image and prompt\n",
        "        response = multimodal_model.generate_content([prompt, img])\n",
        "\n",
        "        # Return the generated text\n",
        "        return response.text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating caption for {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# List of PNG files to process (from Step 2)\n",
        "# Ensure png_files and file_dir are accessible from this cell's scope\n",
        "# If not, you might need to redefine or pass them. Assuming they are global for now.\n",
        "# png_files = [\"daily_puzzle.png\", \"puzzle_1.png\", \"puzzle_2.png\",\"puzzle_3.png\"]\n",
        "# file_dir = '/content/drive/MyDrive/Capstone/Week 5_Advanced_RAG'\n",
        "\n",
        "# Generate captions for the PNG files\n",
        "image_captions = {}\n",
        "for png_file in png_files:\n",
        "    image_path = os.path.join(file_dir, png_file)\n",
        "    caption = generate_image_caption(image_path, prompt=\"Describe the chess puzzle or diagram in this image, focusing on the board position and any notable features.\")\n",
        "    if caption:\n",
        "        image_captions[png_file] = caption\n",
        "        print(f\"\\nCaption for {png_file}: {caption}\")\n",
        "    else:\n",
        "        print(f\"\\nFailed to generate caption for {png_file}\")\n",
        "\n",
        "# Store captions as Document objects for later indexing\n",
        "image_documents = []\n",
        "for filename, caption in image_captions.items():\n",
        "    image_documents.append(Document(page_content=caption, metadata={\"source\": os.path.join(file_dir, filename), \"file_type\": \"image\", \"original_filename\": filename}))\n",
        "\n",
        "print(f\"\\nGenerated captions for {len(image_documents)} images.\")\n",
        "\n",
        "# Now you have image_documents containing captions, ready to be indexed\n",
        "# alongside your text documents in a joint index."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "utPHak1AjfDg",
        "outputId": "90c09750-c8d2-4609-a878-874237728c02"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI configured successfully.\n",
            "\n",
            "Caption for daily_puzzle.png: Based on the image provided, here is a description of the chess puzzle diagram:\n",
            "\n",
            "### General Overview\n",
            "\n",
            "This is a chess puzzle, likely a \"mate-in-X\" problem where Black is to move and win. The position is highly tactical, characterized by an extremely exposed White king in the center of the board, facing a powerful Black attack. The material is equal.\n",
            "\n",
            "### Board Position\n",
            "\n",
            "Here are the positions of the pieces on the board:\n",
            "\n",
            "**White Pieces:**\n",
            "*   **King:** e3\n",
            "*   **Queen:** e6\n",
            "*   **Rooks:** c1, d1\n",
            "*   **Pawns:** a3, e2, f3, g3\n",
            "\n",
            "**Black Pieces:**\n",
            "*   **King:** h8\n",
            "*   **Queen:** h5\n",
            "*   **Rooks:** a8, h6\n",
            "*   **Pawns:** b6, b7, f6, h7\n",
            "\n",
            "### Notable Features\n",
            "\n",
            "*   **Exposed White King:** The most striking feature of the position is the White king's location on e3. It is far from safety, lacking pawn cover, and is a prime target for an attack. Its escape squares are limited.\n",
            "*   **Black's Attacking Battery:** Black has a formidable attacking force aimed at the White king, with a Queen on h5 and a Rook on h6 forming a powerful battery on the h-file.\n",
            "*   **White's Active Defender:** White's main defensive piece is the Queen on e6. It is actively placed, controlling key squares and putting pressure on Black's Rook on h6.\n",
            "*   **Tactical Tension:** The position is filled with tactical possibilities. The puzzle likely involves a forcing sequence of checks or sacrifices from Black to overwhelm White's defenses and checkmate the king.\n",
            "\n",
            "Caption for puzzle_1.png: Based on the image provided, here is a description of the chess puzzle diagram:\n",
            "\n",
            "**Board Position:**\n",
            "\n",
            "This is a mid-game chess position. The last move, indicated by the highlighted square, was Black moving their Queen to d5 (...Qd5). It is now **White's turn to move**.\n",
            "\n",
            "**White's Pieces:**\n",
            "*   **King:** e1\n",
            "*   **Rooks:** a1, h1\n",
            "*   **Knight:** b1\n",
            "*   **Bishops:** c1, c4\n",
            "*   **Pawns:** a2, b2, c2, d2, f2, g2, h2\n",
            "\n",
            "**Black's Pieces:**\n",
            "*   **King:** g8\n",
            "*   **Rooks:** a8, h8\n",
            "*   **Knight:** c6\n",
            "*   **Bishops:** c8, f8\n",
            "*   **Queen:** d5\n",
            "*   **Pawns:** a7, b7, c7, e5, g7, h7\n",
            "\n",
            "**Notable Features and Tactical Situation:**\n",
            "\n",
            "*   **Central Tension:** The most significant feature is the confrontation in the center. The Black Queen has just moved to d5, where it is now directly attacked by White's Bishop on c4.\n",
            "*   **Aggressive Black Queen:** Black's Queen on d5 is a very active piece, controlling key squares and putting pressure on White's position.\n",
            "*   **Active White Bishop:** White's Bishop on c4 is well-placed, not only attacking the queen but also putting pressure on Black's kingside, particularly eyeing the weak f7 square.\n",
            "*   **Development:** Both sides appear to have castled kingside. White's pieces are mostly on their starting squares, with the Bishop on c4 being the most developed piece. Black has developed a knight to c6 and pushed a pawn to e5.\n",
            "*   **The Puzzle:** The core of the puzzle lies in how White should respond to Black's aggressive queen move. The immediate and obvious move is for White to capture the queen (Bxd5), but this would allow Black to recapture with the pawn (exd5), opening the e-file towards the White King. The puzzle likely requires White to find the best continuation, which might be the simple capture or a more complex tactical sequence.\n",
            "\n",
            "Caption for puzzle_2.png: This is a chess puzzle where White is to move and has a decisive attack against the Black king.\n",
            "\n",
            "### Board Position:\n",
            "\n",
            "**White's Pieces:**\n",
            "*   King on g1\n",
            "*   Queen on e4\n",
            "*   Rooks on c1 and e1\n",
            "*   Bishop on d3 (light-squared)\n",
            "*   Bishop on h6 (dark-squared)\n",
            "*   Pawns on a4, c3, d5, f2, g2, h2\n",
            "\n",
            "**Black's Pieces:**\n",
            "*   King on g8\n",
            "*   Queen on c7\n",
            "*   Rooks on a8 and d8\n",
            "*   Knight on b8\n",
            "*   Bishop on f6\n",
            "*   Pawns on a6, b7, d6, f7, g6, h7\n",
            "\n",
            "### Notable Features:\n",
            "\n",
            "*   **White's Dominant Attack:** White has a very strong and well-coordinated attack focused on the Black king. The queen on e4, the two bishops, and the rooks are all poised to strike.\n",
            "*   **The Bishop on h6:** The most notable feature of the position is the White bishop on h6. This piece is a powerful attacking weapon, as it controls the key squares g7 and f8, severely restricting the Black king's movement and preventing it from escaping.\n",
            "*   **Black's Vulnerable King:** The Black king on g8 is extremely exposed. The pawn shield in front of it (f7, g6, h7) has been weakened, particularly the g6 pawn, which is a direct target for White's queen.\n",
            "*   **Forcing Tactic:** The position is ripe for a tactical combination, likely involving a sacrifice to break through Black's defenses and deliver checkmate. The most prominent candidate move is a queen sacrifice with **1. Qxg6+**.\n",
            "*   **Passive Black Pieces:** While Black has a lot of material on the board, many of its pieces are passive or poorly coordinated for defense. The knight on b8, for example, is completely out of the game.\n",
            "\n",
            "Caption for puzzle_3.png: Based on the image, here is a description of the chess puzzle:\n",
            "\n",
            "**Board Position:**\n",
            "\n",
            "This is a late-game chess position. It appears to be White's turn to move.\n",
            "\n",
            "**White's Pieces:**\n",
            "*   **King** on d6\n",
            "*   **Queen** on d2\n",
            "*   **Pawns** on a3, d5, f5, g6, and h2\n",
            "\n",
            "**Black's Pieces:**\n",
            "*   **King** on d8\n",
            "*   **Queen** on b1\n",
            "*   **Rook** on e5\n",
            "*   **Pawns** on a4, f6, g7, and h6\n",
            "\n",
            "**Notable Features:**\n",
            "\n",
            "*   **White is in Check:** The most immediate feature is that the White King on d6 is currently under attack (in check) from the Black Rook on e5. White must make a move to resolve this check.\n",
            "*   **Active Kings:** Both kings are in active, somewhat exposed positions. The White King is unusually far up the board, which is both a strength (as it restricts the Black King) and a weakness (as it's vulnerable to checks).\n",
            "*   **Last Move:** The yellow highlight on square b1 indicates that Black's last move was likely moving the Queen to that square (Qb1).\n",
            "*   **Major Threats:** Black's Rook on e5 and Queen on b1 pose significant threats to White. White's advanced pawns (especially on d5 and g6) and the powerful Queen on d2 create strong counter-threats against the Black King.\n",
            "*   **Puzzle Goal:** The objective for White is to find the best move to escape the check and turn the tables, likely by forcing a checkmate or gaining a decisive material advantage.\n",
            "\n",
            "Generated captions for 4 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate image-only queries\n",
        "\n",
        "# To perform an \"image-only\" query in this text-based RAG setup\n",
        "# (where images are represented by captions), we formulate a text query\n",
        "# that describes the content of an image we want to retrieve.\n",
        "\n",
        "# Choose a query that describes one of the images we captioned.\n",
        "# For example, let's try to retrieve the document related to 'daily_puzzle.png'.\n",
        "# Its caption describes a chess puzzle with an exposed white king and black queen on h5.\n",
        "image_query = \"Describe the chess puzzle with an exposed white king and a black queen on h5.\"\n",
        "\n",
        "print(f\"Performing image-only query: '{image_query}'\")\n",
        "\n",
        "# Use the existing vectorstore (which contains both text chunks and image captions)\n",
        "# to retrieve documents based on the image query.\n",
        "# We expect the document corresponding to 'daily_puzzle.png' (or its caption) to be highly relevant.\n",
        "# Adjust search_kwargs={\"k\": N} to retrieve a suitable number of documents.\n",
        "retrieved_image_docs = vectorstore.similarity_search(image_query, k=3) # Retrieve top 3 documents\n",
        "\n",
        "print(f\"\\nRetrieved {len(retrieved_image_docs)} documents for image-only query:\")\n",
        "for i, doc in enumerate(retrieved_image_docs):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\") # Explicitly print the source file path\n",
        "    print(f\"File Type: {doc.metadata.get('file_type', 'text')}\") # Indicate if it's an image caption\n",
        "    print(\"Content:\")\n",
        "    print(doc.page_content)\n",
        "\n",
        "# Verify if the retrieved documents include the caption for the target image.\n",
        "# You would visually inspect the output to see if the relevant image caption is retrieved.\n",
        "# The 'Source' field should point to the original image file."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar1edUdPl9vk",
        "outputId": "fa604a5f-b545-4b59-bf97-a45bc3922dc6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing image-only query: 'Describe the chess puzzle with an exposed white king and a black queen on h5.'\n",
            "\n",
            "Retrieved 3 documents for image-only query:\n",
            "\n",
            "Document 1:\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/chessgpt.pdf\n",
            "File Type: text\n",
            "Content:\n",
            "Bd7 40. Bc4 f6 41. Bd5 Be8 42. c4 Kd4 43. Kg4 Bg6 44. Kf3 Bh5+ 45. Kf2 Bd1 46.\n",
            "Kg3 Be2 47. c5 Kxc5 48. Be6 Kd4 49. Bf5 Ke3 0-1\n",
            "...\n",
            "Chess Books\n",
            "The following illustrative game is apparently complicated, but it is this in\n",
            "its motives\\nonly.\\nIn reality itis the fight against White’s e4 pawn, which\n",
            "dominates. Shoosmith-\\nNimzowitsch, Ostend, 1907. 1.d4 Nf6 2.c4 d63.Nf3 Nbd7\n",
            "4.Nc3e5 5.e4 Be7 6.Bd3\\n0-0 7.0-0 exd4! (if 7...Re8, then 8.05 and Black\n",
            "\n",
            "Document 2:\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/chessgpt.pdf\n",
            "File Type: text\n",
            "Content:\n",
            "Bd7 40. Bc4 f6 41. Bd5 Be8 42. c4 Kd4 43. Kg4 Bg6 44. Kf3 Bh5+ 45. Kf2 Bd1 46.\n",
            "Kg3 Be2 47. c5 Kxc5 48. Be6 Kd4 49. Bf5 Ke3 0-1\n",
            "...\n",
            "Chess Books\n",
            "The following illustrative game is apparently complicated, but it is this in\n",
            "its motives\\nonly.\\nIn reality itis the fight against White’s e4 pawn, which\n",
            "dominates. Shoosmith-\\nNimzowitsch, Ostend, 1907. 1.d4 Nf6 2.c4 d63.Nf3 Nbd7\n",
            "4.Nc3e5 5.e4 Be7 6.Bd3\\n0-0 7.0-0 exd4! (if 7...Re8, then 8.05 and Black\n",
            "\n",
            "Document 3:\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/chessgpt.pdf\n",
            "File Type: text\n",
            "Content:\n",
            "Bd7 40. Bc4 f6 41. Bd5 Be8 42. c4 Kd4 43. Kg4 Bg6 44. Kf3 Bh5+ 45. Kf2 Bd1 46.\n",
            "Kg3 Be2 47. c5 Kxc5 48. Be6 Kd4 49. Bf5 Ke3 0-1\n",
            "...\n",
            "Chess Books\n",
            "The following illustrative game is apparently complicated, but it is this in\n",
            "its motives\\nonly.\\nIn reality itis the fight against White’s e4 pawn, which\n",
            "dominates. Shoosmith-\\nNimzowitsch, Ostend, 1907. 1.d4 Nf6 2.c4 d63.Nf3 Nbd7\n",
            "4.Nc3e5 5.e4 Be7 6.Bd3\\n0-0 7.0-0 exd4! (if 7...Re8, then 8.05 and Black\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We took an image and created a text caption for it. Taking that text caption, and using it as a query into our vectordb, we were able to find relevant/matching data in text form. It specifically found game references in the chessGPT paper where h5+ occured. This means that a check action was performed when a piece was moved to the h5 square.\n",
        "\n",
        "Our caption query identified this square as important based on the image. The retrieval was able to grab data that was relevant to that context."
      ],
      "metadata": {
        "id": "5zafLmBVnIMs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41ed7887"
      },
      "source": [
        "## Step 5 (Track C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7db89eb3"
      },
      "source": [
        "## Step 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03001945"
      },
      "source": [
        "## Step 7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Reproducibility log\n",
        "\n",
        "# Save configs in rag_adv_run_config.json\n",
        "# - embedding models\n",
        "# - reranker\n",
        "# - chunking\n",
        "# - multimodal pipeline\n",
        "# - guardrails\n",
        "# - retriever (k)\n",
        "\n",
        "# Define the output directory and config file path\n",
        "output_dir = '/content/drive/MyDrive/Capstone/Week 5_Advanced_RAG'\n",
        "config_file_path = os.path.join(output_dir, \"rag_adv_run_config.json\")\n",
        "\n",
        "# Collect configuration details\n",
        "run_config = {\n",
        "    \"embedding_model\": embedding_model_name if 'embedding_model_name' in globals() else \"N/A\",\n",
        "    \"reranker_model\": reranker_model_name if 'reranker_model_name' in globals() else \"N/A\",\n",
        "    \"chunking\": {\n",
        "        \"chunk_size\": chunk_size if 'chunk_size' in globals() else \"N/A\",\n",
        "        \"chunk_overlap\": chunk_overlap if 'chunk_overlap' in globals() else \"N/A\"\n",
        "    },\n",
        "    \"retriever\": {\n",
        "        \"bm25_k\": bm25_retriever.k if 'bm25_retriever' in globals() and hasattr(bm25_retriever, 'k') else \"N/A\",\n",
        "        \"dense_k\": dense_retriever.search_kwargs.get('k', 'N/A') if 'dense_retriever' in globals() and hasattr(dense_retriever, 'search_kwargs') else \"N/A\",\n",
        "        \"ensemble_weights\": ensemble_retriever.weights if 'ensemble_retriever' in globals() and hasattr(ensemble_retriever, 'weights') else \"N/A\"\n",
        "    },\n",
        "    \"multimodal_pipeline\": {\n",
        "        \"image_captioning_model\": \"Gemini-Vision\" if 'genai' in globals() and genai else \"N/A\", # Assuming Gemini was used if genai is configured\n",
        "        \"image_files_processed\": png_files if 'png_files' in globals() else []\n",
        "    },\n",
        "    \"compression_pipeline\": {\n",
        "        \"transformers\": [type(t).__name__ for t in compression_pipeline.transformers] if 'compression_pipeline' in globals() and hasattr(compression_pipeline, 'transformers') else [],\n",
        "        # Add details about LLM if used, e.g., \"llm_compressor_model\": llm_name if 'llm_name' in globals() else \"N/A\"\n",
        "    },\n",
        "    \"guardrails\": \"Skipped as per plan\" # Based on the user skipping Step 5\n",
        "}\n",
        "\n",
        "# Check if the config file exists and load existing data (optional, depends if you want to append or overwrite)\n",
        "# For reproducibility log, overwriting is usually desired to reflect the final state.\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(config_file_path), exist_ok=True)\n",
        "\n",
        "# Save the configuration to the JSON file\n",
        "with open(config_file_path, 'w') as f:\n",
        "    json.dump(run_config, f, indent=4)\n",
        "\n",
        "print(f\"Run configuration saved to {config_file_path}\")\n",
        "print(json.dumps(run_config, indent=4))\n",
        "\n",
        "# This marks the completion of the steps outlined in the plan."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynoxOAaMqfOZ",
        "outputId": "1de3a1be-fbc9-4a98-a500-a957693f2080"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run configuration saved to /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/rag_adv_run_config.json\n",
            "{\n",
            "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
            "    \"reranker_model\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
            "    \"chunking\": {\n",
            "        \"chunk_size\": 500,\n",
            "        \"chunk_overlap\": 100\n",
            "    },\n",
            "    \"retriever\": {\n",
            "        \"bm25_k\": 4,\n",
            "        \"dense_k\": 4,\n",
            "        \"ensemble_weights\": [\n",
            "            0.5,\n",
            "            0.5\n",
            "        ]\n",
            "    },\n",
            "    \"multimodal_pipeline\": {\n",
            "        \"image_captioning_model\": \"Gemini-Vision\",\n",
            "        \"image_files_processed\": [\n",
            "            \"daily_puzzle.png\",\n",
            "            \"puzzle_1.png\",\n",
            "            \"puzzle_2.png\",\n",
            "            \"puzzle_3.png\"\n",
            "        ]\n",
            "    },\n",
            "    \"compression_pipeline\": {\n",
            "        \"transformers\": [\n",
            "            \"EmbeddingsRedundantFilter\"\n",
            "        ]\n",
            "    },\n",
            "    \"guardrails\": \"Skipped as per plan\"\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}