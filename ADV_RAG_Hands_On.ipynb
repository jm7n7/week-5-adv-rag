{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1qPKTFP_NDB4O6l9hsH9vq9XLgzWrdL6L",
      "authorship_tag": "ABX9TyMgD1zPLTFIAn2uy4VshcjC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jm7n7/week-5-adv-rag/blob/main/ADV_RAG_Hands_On.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Track A, B, C: Reranking & Context Optimization | Multimodal RAG | Evaluation & Guardrails**"
      ],
      "metadata": {
        "id": "wCQWk7dibv4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Track A: Reranking & Context Optimization\n",
        "*   Implement Reciprocal Rank Fusion (BM25 + dense)\n",
        "*   Add a reranker (e.g., cross-encoder) and/or MMR/passage compression\n",
        "*   Compare Baseline vs. +Rerank vs. +Compression vs. +Both on your project queries\n",
        "##Track B: Multimodal RAG\n",
        "*  Add image/chart support via captions/embeddings (e.g., Gemini-Vision, BLIP2)\n",
        "*   Build a joint index for text + image\n",
        "*  Demonstrate text-only, image-only, and hybrid queries with grounded citations\n",
        "##Track C: Evaluation & Guardrails\n",
        "*   Create an eval set (10-20 project queries) with gold answers + source IDs\n",
        "*   Compute correctness, faithfullness, context precision/recall, latency, token cost\n",
        "*   Add guardrails: citation enforcement, PII redaction, refusal template\n",
        "*   Report Before vs. after guardrails"
      ],
      "metadata": {
        "id": "gnQn_ZmGxBQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Install & Setup\n",
        "*   Install\n",
        "    - numpy\n",
        "    - pandas\n",
        "    - matplotlib\n",
        "    - sentence-transformers\n",
        "    - faiss\n",
        "    - langchain\n",
        "    - openai\n",
        "*   Log environment to env_rag_adv.json\n",
        "##2. Load Your Project Materials\n",
        "*   Use the same documents from week 4 (optional add new documents)\n",
        "    - PDFs (research papers, survey articles, datasets)\n",
        "    - Text/Markdown notes\n",
        "*   Include 2-3 images / charts for Track B\n",
        "##3. Retrieval Upgrades (Track A)\n",
        "*   Implement RRF (BM25 + dense)\n",
        "    - Add reranker + compression\n",
        "*   Log\n",
        "    - Recall@k\n",
        "    - latency\n",
        "    - avg context length\n",
        "    - token cost\n",
        "##4. Multimodal Retrieval (Track B)\n",
        "*   Caption / encode images with CLIP/BLIP2/Gemini-Vision\n",
        "*   Show at least _one image-only query_ retrieving a relevant chart with citations\n",
        "##5. Evaluation & Guardrails (Track C)\n",
        "*   Build eval_queries.jsonl\n",
        "*   Compute\n",
        "    - correctness/faithfullness\n",
        "    - latency before guardrails\n",
        "    - latency after guardrails\n",
        "*   Include at least _one adversarial/unsafe/PII query_ to test guardrails\n",
        "##6. Ablation Study\n",
        "*   Fill ablation_results.csv:\n",
        "    - Baseline\n",
        "    - +Rerank\n",
        "    - +Compression\n",
        "    - +Multimodal\n",
        "    - +Guardrails\n",
        "*   Plot recall versus latency using matplotlib\n",
        "##7. Reproducibility log\n",
        "*   Save configs in rag_adv_run_config.json\n",
        "    - embedding models\n",
        "    - reranker\n",
        "    - chunking\n",
        "    - multimodal pipeline\n",
        "    - guardrails\n",
        "    - retriever (k)"
      ],
      "metadata": {
        "id": "ogq5iHOYcFZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1"
      ],
      "metadata": {
        "id": "Sc9S96QKiVFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install\n",
        "%pip install langchain chromadb sentence-transformers transformers langchain-community pypdf rank_bm25"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rsnmQWYCiSAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cfb1ab6-586b-48fe-c3fc-8b7aa657d3db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.1.0)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.12/dist-packages (0.2.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.28)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.17.4)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "import platform\n",
        "import chromadb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import sentence_transformers\n",
        "import matplotlib.pyplot as plt\n",
        "#\n",
        "from rank_bm25 import BM25Okapi\n",
        "from google.colab import userdata\n",
        "from langchain.schema import Document\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings.sentence_transformer import (\n",
        "    SentenceTransformerEmbeddings,\n",
        ")\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
        "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "#\n",
        "try:\n",
        "    import torch\n",
        "    torch_v = torch.__version__\n",
        "    cuda_ok = torch.cuda.is_available()\n",
        "    device_name = torch.cuda.get_device_name(0) if cuda_ok else \"CPU\"\n",
        "except:\n",
        "    torch_v, cuda_ok, device_name = \"N/A\", False, \"CPU\""
      ],
      "metadata": {
        "id": "XWGR8zoni9wX"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b0a4d52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c472c8f-5bf3-419e-fd05-9cf898591371"
      },
      "source": [
        "# Log versions\n",
        "env_info = {\n",
        "    \"python\": sys.version,\n",
        "    \"platform\": platform.platform(),\n",
        "    \"torch\": torch_v,\n",
        "    \"cuda\": cuda_ok,\n",
        "    \"device\": device_name,\n",
        "    \"transformers\": transformers.__version__,\n",
        "    \"sentence_transformers\": sentence_transformers.__version__,\n",
        "    \"chromadb\": chromadb.__version__,\n",
        "    \"numpy\": np.__version__,\n",
        "    \"pandas\": pd.__version__,\n",
        "}\n",
        "\n",
        "# Save results in env_rag_adv.json\n",
        "output_dir = '/content/drive/MyDrive/Capstone/Week 5_Advanced_RAG'\n",
        "file_path = os.path.join(output_dir, \"env_rag_adv.json\")\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "# Check if the file exists and load existing data\n",
        "existing_data = {}\n",
        "if os.path.exists(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            existing_data = json.load(f)\n",
        "    except json.JSONDecodeError:\n",
        "        existing_data = {} # Handle empty or invalid JSON\n",
        "\n",
        "# Update existing data with new environment info\n",
        "existing_data.update(env_info)\n",
        "\n",
        "with open(file_path, 'w') as f:\n",
        "    json.dump(existing_data, f, indent=4)\n",
        "\n",
        "print(f\"Environment information saved to {file_path}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment information saved to /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/env_rag_adv.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2"
      ],
      "metadata": {
        "id": "gyblUWHuj6RW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90cbc9d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8791c524-12cb-4d96-e040-7cf46460a7d9"
      },
      "source": [
        "# Define the directory where the files are located\n",
        "file_dir = '/content/drive/MyDrive/Capstone/Week 5_Advanced_RAG'\n",
        "\n",
        "# List of PDF files to load for text-based RAG\n",
        "pdf_files = [\"maia-2.pdf\", \"Amortized_chess.pdf\", \"chessgpt.pdf\"]\n",
        "\n",
        "# List of PNG files to be used for multimodal RAG (will be processed separately)\n",
        "png_files = [\"daily_puzzle.png\", \"puzzle_1.png\", \"puzzle_2.png\",\"puzzle_3.png\"]\n",
        "\n",
        "# Load the PDF documents using PyPDFLoader\n",
        "pdf_documents = []\n",
        "for pdf_file in pdf_files:\n",
        "    file_path = os.path.join(file_dir, pdf_file)\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    pdf_documents.extend(loader.load())\n",
        "\n",
        "png_documents = []\n",
        "for png_file in png_files:\n",
        "    file_path = os.path.join(file_dir, png_file)\n",
        "    # Create a simple Document object with file path as content and source\n",
        "    png_documents.append(Document(page_content=f\"Image file: {png_file}\", metadata={\"source\": file_path, \"file_type\": \"png\"}))\n",
        "\n",
        "# Combine all documents (PDFs and placeholder PNGs)\n",
        "all_documents = pdf_documents + png_documents\n",
        "\n",
        "print(f\"Loaded {len(pdf_documents)} PDF documents.\")\n",
        "print(f\"Listed and created placeholder documents for {len(png_documents)} PNG files for future multimodal processing.\")\n",
        "print(f\"Total documents (PDFs + PNG placeholders): {len(all_documents)}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 99 PDF documents.\n",
            "Listed and created placeholder documents for 4 PNG files for future multimodal processing.\n",
            "Total documents (PDFs + PNG placeholders): 103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dea875ae"
      },
      "source": [
        "## Step 3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replicate Week 4 work"
      ],
      "metadata": {
        "id": "OMF-VbZe_5sW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00be2770",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dddb823a-ae81-4265-af4c-66129f9a5d49"
      },
      "source": [
        "# Define chunking parameters\n",
        "chunk_size = 500\n",
        "chunk_overlap = 100\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap\n",
        ")\n",
        "\n",
        "# Split the documents into chunks\n",
        "chunks = text_splitter.split_documents(all_documents) # Use 'all_documents' from Step 2\n",
        "\n",
        "# Preview chunk count and first chunk\n",
        "print(f\"Created {len(chunks)} chunks.\")\n",
        "if chunks:\n",
        "    print(\"\\nFirst chunk:\")\n",
        "    print(chunks[0].page_content)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 813 chunks.\n",
            "\n",
            "First chunk:\n",
            "Maia-2: A Unified Model for Human-AI Alignment in\n",
            "Chess\n",
            "Zhenwei Tang\n",
            "University of Toronto\n",
            "josephtang@cs.toronto.edu\n",
            "Difan Jiao\n",
            "University of Toronto\n",
            "difanjiao@cs.toronto.edu\n",
            "Reid McIlroy-Young\n",
            "Harvard University\n",
            "reidmcy@seas.harvard.edu\n",
            "Jon Kleinberg\n",
            "Cornell University\n",
            "kleinberg@cornell.edu\n",
            "Siddhartha Sen\n",
            "Microsoft Research\n",
            "sidsen@microsoft.com\n",
            "Ashton Anderson\n",
            "University of Toronto\n",
            "ashton@cs.toronto.edu\n",
            "Abstract\n",
            "There are an increasing number of domains in which artificial intelligence (AI)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the embedding model\n",
        "embedding_model_name = \"all-MiniLM-L6-v2\"\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "# Create the Chroma vector database\n",
        "# We'll store the database in the same output directory\n",
        "db_dir = os.path.join(output_dir, \"chroma_db\")\n",
        "vectorstore = Chroma.from_documents(chunks, embedding_function, persist_directory=db_dir)\n",
        "\n",
        "# Create a retriever from the vector store\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "# Verify retrieval with a sample query\n",
        "sample_query = \"What is the main idea of the Maia-2 paper?\"\n",
        "docs = retriever.invoke(sample_query)\n",
        "\n",
        "print(f\"\\nSample Query: {sample_query}\")\n",
        "print(f\"\\nRetrieved {len(docs)} documents:\")\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content)\n",
        "\n",
        "# Save embedding model and retriever k value to rag_run_config.json\n",
        "file_path = os.path.join(output_dir, \"rag_run_config.json\")\n",
        "\n",
        "# Check if the file exists and load existing data\n",
        "existing_data = {}\n",
        "if os.path.exists(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            existing_data = json.load(f)\n",
        "    except json.JSONDecodeError:\n",
        "        existing_data = {} # Handle empty or invalid JSON\n",
        "\n",
        "# Update existing data with new information\n",
        "existing_data.update({\n",
        "    \"embedding_model\": embedding_model_name,\n",
        "    \"retriever_k\": 4\n",
        "})\n",
        "\n",
        "# Save the updated data to the file\n",
        "with open(file_path, 'w') as f:\n",
        "    json.dump(existing_data, f, indent=4)\n",
        "\n",
        "print(f\"\\nConfiguration updated in {file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SaUa81U8tLj",
        "outputId": "324c46d1-26a1-48eb-97f4-876af9cb6e5b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Query: What is the main idea of the Maia-2 paper?\n",
            "\n",
            "Retrieved 4 documents:\n",
            "\n",
            "Document 1:\n",
            "interact with chess positions to produce the moves humans make. Unlike previous models, Maia-2\n",
            "only requires the current board position as input (as opposed to six), which dramatically reduces\n",
            "training time and increases flexibility (e.g. for applying the model in non-game contexts where there\n",
            "may be no 6-board history). In addition to policy and value heads like in previous work, we also add\n",
            "an additional auxiliary information head that helps the model learn a deeper understanding of human\n",
            "\n",
            "Document 2:\n",
            "important dimension is prediction coherence as skill varies. A central drawback of Maia-1 is that it\n",
            "8\n",
            "\n",
            "Document 3:\n",
            "Maia-2subset. Maia-2 differs from Maia-1 in two main ways: it has a different architecture and it\n",
            "has access to more training data. To control for the difference in training data and isolate the effects\n",
            "of our architecture, we create Maia-2 subset which has access to the exact same training data that\n",
            "Maia-1 was developed with. Comparing the two, we see that Maia-2subset matches or outperforms\n",
            "all baselines and alternate models. Recall that Maia-2 and Maia-2subset don’t have the recent history\n",
            "\n",
            "Document 4:\n",
            "Maia-2 can mimic weaker players to whom the puzzle is hard to solve, while stronger Maia-2 with\n",
            "skill level configured above or equal to 1500 can successfully solve the puzzle. However, Maia 1100\n",
            "surprisingly solved the puzzle, while the stronger Maia-1 models, e.g., Maia 1700 failed to make\n",
            "the optimal move. Therefore, in the considered case, as opposed to Maia-1, Maia-2 yields smooth\n",
            "predictions provided that its treatment of this position is monotonic and transitional.\n",
            "14\n",
            "\n",
            "Configuration updated in /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/rag_run_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Week 5 upgrades"
      ],
      "metadata": {
        "id": "XRazebdaCCHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of document texts for BM25\n",
        "document_texts = [doc.page_content for doc in pdf_documents]\n",
        "\n",
        "# Initialize BM25Retriever\n",
        "bm25_retriever = BM25Retriever.from_texts(document_texts, metadatas=[doc.metadata for doc in pdf_documents])\n",
        "bm25_retriever.k = 4 # Set a default k value for BM25\n",
        "\n",
        "print(\"BM25 Retriever initialized.\")\n",
        "\n",
        "# Use the existing vectorstore for the dense retriever\n",
        "dense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4}) # Set k for dense\n",
        "\n",
        "print(\"Dense Retriever initialized.\")\n",
        "\n",
        "# Initialize the EnsembleRetriever with BM25 and dense retrievers\n",
        "# weights can be adjusted based on desired contribution of each retriever\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, dense_retriever], weights=[0.5, 0.5])\n",
        "\n",
        "print(\"Ensemble Retriever (RRF) initialized.\")\n",
        "\n",
        "# Test the RRF retriever with a sample query\n",
        "sample_query = \"What is the main idea of the Maia-2 paper?\"\n",
        "docs_rrf = ensemble_retriever.invoke(sample_query)\n",
        "\n",
        "print(f\"\\nSample Query with RRF: {sample_query}\")\n",
        "print(f\"\\nRetrieved {len(docs_rrf)} documents using RRF:\")\n",
        "for i, doc in enumerate(docs_rrf):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content)\n",
        "    print(f\"Source: {doc.metadata.get('source')}\") # Include source information if available"
      ],
      "metadata": {
        "id": "mbD3GPdB-L2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "f2235514-9ef8-4a59-d204-5612b8fca1c9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 Retriever initialized.\n",
            "Dense Retriever initialized.\n",
            "Ensemble Retriever (RRF) initialized.\n",
            "\n",
            "Sample Query with RRF: What is the main idea of the Maia-2 paper?\n",
            "\n",
            "Retrieved 8 documents using RRF:\n",
            "\n",
            "Document 1:\n",
            "Figure 4: Maia-2’s chess concept recognition as a function of skill level, as measured by linear\n",
            "activation probes right before (blue) and after (orange) skill-aware attention. (a) Stockfish overall\n",
            "board evaluation for middle-game positions. (b) Stockfish evaluation of middle-game bonuses and\n",
            "penalties to pieces for white. (c) Does the active player own two bishops? (d) Can the active player\n",
            "capture the opponent’s queen?\n",
            "subtle nuances. We now turn our focus to a critical question: does Maia-2 vary in its ability to capture\n",
            "human chess concepts when given different skill levels? Following the chess concepts probing\n",
            "strategy for AlphaZero [22], we show how Maia-2’s grasp of various concepts varies with skill. The\n",
            "left two plots in Figure 4 show concepts for which Maia-2 clearly distinguishes between skill levels,\n",
            "with higher-skill players paying more attention to them than lower-skill players. These are general\n",
            "board evaluations as given by Stockfish [ 6], or aggregate piece values. Note that pre-skill-aware\n",
            "attention is always flat because by construction it cannot vary with skill, since skill-aware attention has\n",
            "not been applied yet. The two plots on the right depict concepts that live closer to fundamental chess\n",
            "rules, and as such are less dependent on player skill. For skill-dependent concepts, the figures reveal\n",
            "an increasing trend in mastery level after skill-aware attention, aligning with the increase in dedicated\n",
            "skill levels. Meanwhile, the model’s mastery level decreases after passing through the skill-aware\n",
            "attention modules, potentially adjusting for the imperfections of human players. Conversely, the\n",
            "skill-aware attention blocks are not responsive to skill-independent concepts.\n",
            "5 Discussion\n",
            "Human Study. In addition to human move matching, we also consider engagement, another\n",
            "dimension of human study. In particular, we implement a randomized experiment on Lichess:\n",
            "human players challenge our bots, and we randomize whether players play against Maia-1 or Maia-2.\n",
            "Our result is that our higher move-matching and our vastly improved coherence, across all skill\n",
            "levels, come at no cost to human subject engagement, and in fact slightly increase engagement:\n",
            "players rematch Maia-2 almost 1 percentage point more than Maia-1 (41.2% vs. 40.3%). Although\n",
            "engagement is not our main objective, this is further promising evidence that we have achieved a\n",
            "more human-aligned model that coherently captures human style across different skill levels.\n",
            "Ethical Considerations. We believe Maia-2 poses limited risk while offering large potential benefits.\n",
            "Our data is highly aggregated, with almost 1 billion games being used for training, and chess as a\n",
            "domain is generally low-risk. Meanwhile, helping people improve in chess could lead to increased\n",
            "cognitive skills, confidence boosts, and help with general life satisfaction. Our vision is for Maia-2 to\n",
            "power AI partners and training aids; it cannot currently replace skilled human tutors and coaches.\n",
            "Limitation. Our work has limitations. First, we are excited by the applications that Maia-2 will\n",
            "enable, such as more relatable AI partners and AI-powered learning aids, the development of which is\n",
            "out of scope for the current work. Maia-2 does not yet incorporate search, although previous work has\n",
            "demonstrated that with proper regularization it can help improve move prediction performance [26].\n",
            "Relatedly, we group the strongest players in a single bucket, although modeling the very best players\n",
            "in the world remains difficult due to the complexity and depth of their moves.\n",
            "10\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 2:\n",
            "interact with chess positions to produce the moves humans make. Unlike previous models, Maia-2\n",
            "only requires the current board position as input (as opposed to six), which dramatically reduces\n",
            "training time and increases flexibility (e.g. for applying the model in non-game contexts where there\n",
            "may be no 6-board history). In addition to policy and value heads like in previous work, we also add\n",
            "an additional auxiliary information head that helps the model learn a deeper understanding of human\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 3:\n",
            "1200 1600 2000\n",
            "Active Player's Skill Level\n",
            "1200 1600 2000\n",
            "Opponent Player's Skill Level\n",
            "48.0 47.5 47.3 46.1\n",
            "49.2 48.4 48.8 48.1 47.5\n",
            "49.5 49.3 49.0 49.1 48.4 47.8\n",
            "50.1 50.0 49.9 49.8 49.2 48.7 48.6\n",
            "50.4 50.8 50.6 50.4 50.1 50.1 49.6\n",
            "51.0 50.9 51.5 50.9 51.0 50.7 50.8\n",
            "50.2 50.5 51.5 51.6 51.5 51.5 51.4\n",
            "50.5 51.9 51.6 52.0 50.8 51.8\n",
            "51.5 51.9 52.2 52.2 50.6\n",
            "51.6 51.8 52.0 52.8\n",
            "Maia 1100\n",
            "1200 1600 2000\n",
            "Active Player's Skill Level\n",
            "51.1 52.0 52.5 52.9\n",
            "51.1 51.5 52.8 52.9 53.3\n",
            "49.4 51.2 51.6 52.9 53.0 54.0\n",
            "49.0 49.9 51.4 52.0 52.7 53.5 54.4\n",
            "47.8 49.5 50.9 51.6 52.2 53.3 54.2\n",
            "47.5 48.3 50.4 50.5 51.8 52.9 53.9\n",
            "46.2 47.3 49.6 50.3 50.7 52.0 53.0\n",
            "46.3 48.7 49.2 50.3 49.8 52.3\n",
            "47.7 48.4 49.4 50.2 49.0\n",
            "47.3 48.3 48.8 50.2\n",
            "Maia 1900\n",
            "1200 1600 2000\n",
            "Active Player's Skill Level\n",
            "52.4 53.1 53.8 53.6\n",
            "52.7 53.3 54.2 54.2 54.3\n",
            "51.5 53.0 53.2 54.3 54.5 55.0\n",
            "51.6 52.2 53.4 53.8 54.4 54.9 55.7\n",
            "51.1 52.1 53.1 53.7 54.2 55.2 55.6\n",
            "51.1 51.5 53.4 53.1 54.0 54.8 55.6\n",
            "50.0 50.6 52.8 53.4 53.3 54.5 55.1\n",
            "50.2 52.2 52.6 53.4 52.3 54.8\n",
            "51.4 52.0 52.9 53.3 52.2\n",
            "51.2 52.1 52.4 53.9\n",
            "Maia-2\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "Move Prediction Accuracy\n",
            "Figure 2: Move prediction accuracy across diverse skill levels. Colors represent performance, with\n",
            "warmer tones indicating higher accuracy.\n",
            "4.1 Move Prediction Accuracy\n",
            "Maia-2. In Table 1, we show the top-1 move prediction accuracy of all models across all groups of\n",
            "players on the Maia-1 Testset. Maia-2 demonstrates strong and consistent performance across all\n",
            "skill levels, surpassing all baselines. Specifically, despite Maia-1 models being specifically trained to\n",
            "mimic chess moves by players at specific skill levels, Maia-2 emerges as a unified one-for-all model\n",
            "that is consistently effective across the entire spectrum of chess skills. The largest improvement is\n",
            "on Advanced players, where Maia-2 gains 1.5 percentage points over the nearest competitor (Maia\n",
            "1500). When averaging across skill levels, Maia-2 outperforms all other models by almost 2 full\n",
            "percentage points in overall accuracy. Note that the ceiling accuracy of human move prediction is far\n",
            "below 100% given the randomness and diversity of human decisions—even thesame player won’t\n",
            "always make the same decision when faced with the same position. Our 2 percentage point gain is\n",
            "substantial considering that the difference between Maia-1 and Leela, the previous state-of-the-art\n",
            "model for this task and a traditional chess engine not trained for this task at all, is only 6 percentage\n",
            "points. Furthermore, Maia-1 is essentially a mixture of 9 experts targeting the specific players’\n",
            "skill level, where each expert has 10.3M parameters. Regarding the routing function to select the\n",
            "best-performing expert as a nonparameterized function, Maia-1 has 92M parameters in total. Maia-2,\n",
            "on the other hand, is a one-for-all model with 23.3M parameters under our default settings. Therefore,\n",
            "Maia-2 achieves better human move prediction accuracy with even much fewer trainable parameters.\n",
            "Baseline models. Both Maia-2 and Maia-1 significantly outperform Stockfish and Leela, typically by\n",
            "5–15 percentage points. Note that Stockfish and Leela aim to play optimal chess (as most humans do\n",
            "too), and only “predict” human moves when their approximations to optimality happen to overlap\n",
            "with those of human players. However, we compare to these traditional chess engines because besides\n",
            "Maia-1, there are still the default method of creating “human-like” AI agents. The accuracy gap\n",
            "between Maia-1 architectures and traditional chess engines demonstrates the necessity of developing\n",
            "specialized models to mimic human chess moves.\n",
            "Maia-2subset. Maia-2 differs from Maia-1 in two main ways: it has a different architecture and it\n",
            "has access to more training data. To control for the difference in training data and isolate the effects\n",
            "of our architecture, we create Maia-2 subset which has access to the exact same training data that\n",
            "Maia-1 was developed with. Comparing the two, we see that Maia-2subset matches or outperforms\n",
            "all baselines and alternate models. Recall that Maia-2 and Maia-2subset don’t have the recent history\n",
            "passed as input to them, yet still achieve state-of-the-art results. It is important to note that each\n",
            "Maia-1 model is specifically trained for its respective skill level, relying solely on games where the\n",
            "active and opponent skill levels match for its training data. On the contrary, the unified modeling\n",
            "approach with skill-aware attention of Maia-2subset allows it to utilize a broader spectrum of games,\n",
            "featuring a variety of skill-level pairings, for training purposes. Consequently, while both Maia-1 and\n",
            "Maia-2subset draw from the same source dataset, Maia-2subset can leverage a significantly larger portion\n",
            "of this data for its training, improving its learning and predictive capabilities. The improvement from\n",
            "Maia-2subset to Maia-2 underscores the importance of extensive training with vast datasets. A broader\n",
            "range of games provides Maia-2 with access to more comprehensive and nuanced patterns in human\n",
            "chess moves. Using Maia-2 subset as a comparison, we can determine the relative contributions of\n",
            "model architecture and training data to Maia-2’s 1.9 percentage point gap over its nearest rival (Maia\n",
            "1500). This calculation suggests that 73% of the increase in performance is due to the architecture\n",
            "improvements and 27% is due to increased training data.\n",
            "7\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 4:\n",
            "important dimension is prediction coherence as skill varies. A central drawback of Maia-1 is that it\n",
            "8\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 5:\n",
            "• At submission time, to preserve anonymity, the authors should release anonymized\n",
            "versions (if applicable).\n",
            "• Providing as much information as possible in supplemental material (appended to the\n",
            "paper) is recommended, but including URLs to data and code is permitted.\n",
            "6. Experimental Setting/Details\n",
            "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-\n",
            "parameters, how they were chosen, type of optimizer, etc.) necessary to understand the\n",
            "results?\n",
            "Answer: [Yes]\n",
            "Justification: We include dataset details and hyperparameter settings in Section 4 and\n",
            "Appendix B.\n",
            "Guidelines:\n",
            "• The answer NA means that the paper does not include experiments.\n",
            "• The experimental setting should be presented in the core of the paper to a level of detail\n",
            "that is necessary to appreciate the results and make sense of them.\n",
            "• The full details can be provided either with the code, in appendix, or as supplemental\n",
            "material.\n",
            "7. Experiment Statistical Significance\n",
            "Question: Does the paper report error bars suitably and correctly defined or other appropriate\n",
            "information about the statistical significance of the experiments?\n",
            "Answer: [No]\n",
            "Justification: We train Maia-2 with a huge amount (9.1B) of chess positions. Therefore, it is\n",
            "hard to evaluate Maia-2 multiple times with different train/test splits.\n",
            "Guidelines:\n",
            "• The answer NA means that the paper does not include experiments.\n",
            "• The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\n",
            "dence intervals, or statistical significance tests, at least for the experiments that support\n",
            "the main claims of the paper.\n",
            "• The factors of variability that the error bars are capturing should be clearly stated (for\n",
            "example, train/test split, initialization, random drawing of some parameter, or overall\n",
            "run with given experimental conditions).\n",
            "• The method for calculating the error bars should be explained (closed form formula,\n",
            "call to a library function, bootstrap, etc.)\n",
            "• The assumptions made should be given (e.g., Normally distributed errors).\n",
            "• It should be clear whether the error bar is the standard deviation or the standard error\n",
            "of the mean.\n",
            "• It is OK to report 1-sigma error bars, but one should state it. The authors should\n",
            "preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\n",
            "of Normality of errors is not verified.\n",
            "• For asymmetric distributions, the authors should be careful not to show in tables or\n",
            "figures symmetric error bars that would yield results that are out of range (e.g. negative\n",
            "error rates).\n",
            "• If error bars are reported in tables or plots, The authors should explain in the text how\n",
            "they were calculated and reference the corresponding figures or tables in the text.\n",
            "8. Experiments Compute Resources\n",
            "Question: For each experiment, does the paper provide sufficient information on the com-\n",
            "puter resources (type of compute workers, memory, time of execution) needed to reproduce\n",
            "the experiments?\n",
            "Answer: [Yes]\n",
            "Justification: We report the required computational resources in Appendix B.\n",
            "Guidelines:\n",
            "23\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 6:\n",
            "Maia-2subset. Maia-2 differs from Maia-1 in two main ways: it has a different architecture and it\n",
            "has access to more training data. To control for the difference in training data and isolate the effects\n",
            "of our architecture, we create Maia-2 subset which has access to the exact same training data that\n",
            "Maia-1 was developed with. Comparing the two, we see that Maia-2subset matches or outperforms\n",
            "all baselines and alternate models. Recall that Maia-2 and Maia-2subset don’t have the recent history\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 7:\n",
            "Prompt\n",
            "What determines which player starts first in a chess game? (Answer: white)\n",
            "ChessGPT-Base\n",
            "• White always moves first.\n",
            "• White moves first.\n",
            "• In chess, the player with the white pieces always moves first.\n",
            "ChessGPT-Chat\n",
            "• White and Black are both considered to have the advantage, and the player who is\n",
            "the first to move (the white player) is always the first player to move.\n",
            "• White is determined by the player with the light-colored pieces, also known as the\n",
            "\"white\" player.\n",
            "• In a game of chess, the player with the white pieces should start first.\n",
            "RedPajama\n",
            "• The first player to move is determined by the order in which the players are seated.\n",
            "The first player is seated to the left of the board.\n",
            "• The first player to move is determined by a sequence of moves called the \"opening\".\n",
            "This sequence of moves is determined by the player who moves first.\n",
            "• The first player to move is the one who moves first in the first game.\n",
            "Human Error Analysis\n",
            "• ChessGPT-Base: All answers clearly state that white always goes first, which is\n",
            "accurate.\n",
            "• ChessGPT-Chat: All answers mentioned that white or the player using white pawns\n",
            "should move first, which is also accurate. However, the \"both white and black are\n",
            "considered to have an advantage\" mentioned in the first answer is inaccurate,\n",
            "because usually white is considered to have a small opening advantage.\n",
            "• RedPajama: None of the answers explicitly state that white goes first, and the first\n",
            "and second answers are completely inaccurate. The third answer was vague and\n",
            "unclear.\n",
            "Table 20: Question on starting player.\n",
            "39\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/chessgpt.pdf\n",
            "\n",
            "Document 8:\n",
            "Maia-2 can mimic weaker players to whom the puzzle is hard to solve, while stronger Maia-2 with\n",
            "skill level configured above or equal to 1500 can successfully solve the puzzle. However, Maia 1100\n",
            "surprisingly solved the puzzle, while the stronger Maia-1 models, e.g., Maia 1700 failed to make\n",
            "the optimal move. Therefore, in the considered case, as opposed to Maia-1, Maia-2 yields smooth\n",
            "predictions provided that its treatment of this position is monotonic and transitional.\n",
            "14\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the reranker model and tokenizer\n",
        "reranker_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
        "reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name)\n",
        "\n",
        "# Define a function to rerank documents\n",
        "def rerank_documents(query, documents, top_n=5):\n",
        "    # Return empty list if no documents are provided\n",
        "    if not documents:\n",
        "        return []\n",
        "\n",
        "    # Create pairs of query and document content for the cross-encoder\n",
        "    pairs = [[query, doc.page_content] for doc in documents]\n",
        "\n",
        "    # Use the reranker model to get scores for each pair\n",
        "    with torch.no_grad():\n",
        "        inputs = reranker_tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "        scores = reranker_model(**inputs).logits.squeeze(-1)\n",
        "\n",
        "    # Sort documents based on the reranker scores in descending order\n",
        "    sorted_docs = [documents[i] for i in scores.argsort(descending=True)]\n",
        "\n",
        "    # Print a message indicating reranking is done\n",
        "    print(f\"\\nReranked documents using {reranker_model_name}.\")\n",
        "\n",
        "    # Return the top_n reranked documents\n",
        "    return sorted_docs[:top_n]\n",
        "\n",
        "# Test the reranking function with RRF results\n",
        "reranked_rrf_docs = rerank_documents(sample_query, docs_rrf, top_n=5)\n",
        "\n",
        "# Print the top 5 reranked RRF documents\n",
        "print(f\"\\nTop 5 Reranked RRF documents:\")\n",
        "for i, doc in enumerate(reranked_rrf_docs):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content)\n",
        "    print(f\"Source: {doc.metadata.get('source')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TGmmiYuTKyk3",
        "outputId": "c70cdb07-ce2b-468a-a252-040bc9b5fafc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reranked documents using cross-encoder/ms-marco-MiniLM-L-6-v2.\n",
            "\n",
            "Top 5 Reranked RRF documents:\n",
            "\n",
            "Document 1:\n",
            "Maia-2subset. Maia-2 differs from Maia-1 in two main ways: it has a different architecture and it\n",
            "has access to more training data. To control for the difference in training data and isolate the effects\n",
            "of our architecture, we create Maia-2 subset which has access to the exact same training data that\n",
            "Maia-1 was developed with. Comparing the two, we see that Maia-2subset matches or outperforms\n",
            "all baselines and alternate models. Recall that Maia-2 and Maia-2subset don’t have the recent history\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 2:\n",
            "Maia-2 can mimic weaker players to whom the puzzle is hard to solve, while stronger Maia-2 with\n",
            "skill level configured above or equal to 1500 can successfully solve the puzzle. However, Maia 1100\n",
            "surprisingly solved the puzzle, while the stronger Maia-1 models, e.g., Maia 1700 failed to make\n",
            "the optimal move. Therefore, in the considered case, as opposed to Maia-1, Maia-2 yields smooth\n",
            "predictions provided that its treatment of this position is monotonic and transitional.\n",
            "14\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 3:\n",
            "Figure 4: Maia-2’s chess concept recognition as a function of skill level, as measured by linear\n",
            "activation probes right before (blue) and after (orange) skill-aware attention. (a) Stockfish overall\n",
            "board evaluation for middle-game positions. (b) Stockfish evaluation of middle-game bonuses and\n",
            "penalties to pieces for white. (c) Does the active player own two bishops? (d) Can the active player\n",
            "capture the opponent’s queen?\n",
            "subtle nuances. We now turn our focus to a critical question: does Maia-2 vary in its ability to capture\n",
            "human chess concepts when given different skill levels? Following the chess concepts probing\n",
            "strategy for AlphaZero [22], we show how Maia-2’s grasp of various concepts varies with skill. The\n",
            "left two plots in Figure 4 show concepts for which Maia-2 clearly distinguishes between skill levels,\n",
            "with higher-skill players paying more attention to them than lower-skill players. These are general\n",
            "board evaluations as given by Stockfish [ 6], or aggregate piece values. Note that pre-skill-aware\n",
            "attention is always flat because by construction it cannot vary with skill, since skill-aware attention has\n",
            "not been applied yet. The two plots on the right depict concepts that live closer to fundamental chess\n",
            "rules, and as such are less dependent on player skill. For skill-dependent concepts, the figures reveal\n",
            "an increasing trend in mastery level after skill-aware attention, aligning with the increase in dedicated\n",
            "skill levels. Meanwhile, the model’s mastery level decreases after passing through the skill-aware\n",
            "attention modules, potentially adjusting for the imperfections of human players. Conversely, the\n",
            "skill-aware attention blocks are not responsive to skill-independent concepts.\n",
            "5 Discussion\n",
            "Human Study. In addition to human move matching, we also consider engagement, another\n",
            "dimension of human study. In particular, we implement a randomized experiment on Lichess:\n",
            "human players challenge our bots, and we randomize whether players play against Maia-1 or Maia-2.\n",
            "Our result is that our higher move-matching and our vastly improved coherence, across all skill\n",
            "levels, come at no cost to human subject engagement, and in fact slightly increase engagement:\n",
            "players rematch Maia-2 almost 1 percentage point more than Maia-1 (41.2% vs. 40.3%). Although\n",
            "engagement is not our main objective, this is further promising evidence that we have achieved a\n",
            "more human-aligned model that coherently captures human style across different skill levels.\n",
            "Ethical Considerations. We believe Maia-2 poses limited risk while offering large potential benefits.\n",
            "Our data is highly aggregated, with almost 1 billion games being used for training, and chess as a\n",
            "domain is generally low-risk. Meanwhile, helping people improve in chess could lead to increased\n",
            "cognitive skills, confidence boosts, and help with general life satisfaction. Our vision is for Maia-2 to\n",
            "power AI partners and training aids; it cannot currently replace skilled human tutors and coaches.\n",
            "Limitation. Our work has limitations. First, we are excited by the applications that Maia-2 will\n",
            "enable, such as more relatable AI partners and AI-powered learning aids, the development of which is\n",
            "out of scope for the current work. Maia-2 does not yet incorporate search, although previous work has\n",
            "demonstrated that with proper regularization it can help improve move prediction performance [26].\n",
            "Relatedly, we group the strongest players in a single bucket, although modeling the very best players\n",
            "in the world remains difficult due to the complexity and depth of their moves.\n",
            "10\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 4:\n",
            "interact with chess positions to produce the moves humans make. Unlike previous models, Maia-2\n",
            "only requires the current board position as input (as opposed to six), which dramatically reduces\n",
            "training time and increases flexibility (e.g. for applying the model in non-game contexts where there\n",
            "may be no 6-board history). In addition to policy and value heads like in previous work, we also add\n",
            "an additional auxiliary information head that helps the model learn a deeper understanding of human\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 5:\n",
            "important dimension is prediction coherence as skill varies. A central drawback of Maia-1 is that it\n",
            "8\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the EmbeddingsRedundantFilter for compression\n",
        "redundant_filter = EmbeddingsRedundantFilter(embeddings=embedding_function)\n",
        "\n",
        "# Create a DocumentCompressorPipeline with the redundant filter\n",
        "compression_pipeline = DocumentCompressorPipeline(transformers=[redundant_filter])\n",
        "\n",
        "print(\"\\nCompression pipeline initialized with EmbeddingsRedundantFilter.\")\n",
        "\n",
        "# Define a function to retrieve, rerank, and compress documents\n",
        "def retrieve_and_compress(query, rrf_retriever, reranker_function, compressor_pipeline, top_n_rerank=5):\n",
        "    # Perform RRF retrieval\n",
        "    print(f\"\\nPerforming RRF retrieval for query: {query}\")\n",
        "    initial_docs = rrf_retriever.invoke(query)\n",
        "\n",
        "    # Rerank the retrieved documents\n",
        "    print(f\"Reranking {len(initial_docs)} retrieved documents.\")\n",
        "    reranked_docs = reranker_function(query, initial_docs, top_n=top_n_rerank)\n",
        "\n",
        "    # Apply compression to the reranked documents\n",
        "    print(f\"Applying compression to {len(reranked_docs)} reranked documents.\")\n",
        "    compressed_docs = compressor_pipeline.compress_documents(reranked_docs, query=query)\n",
        "\n",
        "    return compressed_docs\n",
        "\n",
        "# Test the combined retrieval and compression process\n",
        "final_retrieved_compressed_docs = retrieve_and_compress(\n",
        "    sample_query, # Use the predefined sample_query\n",
        "    ensemble_retriever, # Use the RRF retriever\n",
        "    rerank_documents, # Use the reranking function\n",
        "    compression_pipeline, # Use the compression pipeline\n",
        "    top_n_rerank=5 # Specify the number of documents to keep after reranking\n",
        ")\n",
        "\n",
        "# Print the final retrieved and compressed documents\n",
        "print(f\"\\nFinal Retrieved and Compressed Documents (RRF -> Reranking -> Compression):\")\n",
        "for i, doc in enumerate(final_retrieved_compressed_docs):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content)\n",
        "    print(f\"Source: {doc.metadata.get('source')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3mTr1tDLH2L",
        "outputId": "6b12416b-1544-41ff-bf4f-0b1dc25e1d2a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Compression pipeline initialized with EmbeddingsRedundantFilter.\n",
            "\n",
            "Performing RRF retrieval for query: What is the main idea of the Maia-2 paper?\n",
            "Reranking 8 retrieved documents.\n",
            "\n",
            "Reranked documents using cross-encoder/ms-marco-MiniLM-L-6-v2.\n",
            "Applying compression to 5 reranked documents.\n",
            "\n",
            "Final Retrieved and Compressed Documents (RRF -> Reranking -> Compression):\n",
            "\n",
            "Document 1:\n",
            "Maia-2subset. Maia-2 differs from Maia-1 in two main ways: it has a different architecture and it\n",
            "has access to more training data. To control for the difference in training data and isolate the effects\n",
            "of our architecture, we create Maia-2 subset which has access to the exact same training data that\n",
            "Maia-1 was developed with. Comparing the two, we see that Maia-2subset matches or outperforms\n",
            "all baselines and alternate models. Recall that Maia-2 and Maia-2subset don’t have the recent history\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 2:\n",
            "Maia-2 can mimic weaker players to whom the puzzle is hard to solve, while stronger Maia-2 with\n",
            "skill level configured above or equal to 1500 can successfully solve the puzzle. However, Maia 1100\n",
            "surprisingly solved the puzzle, while the stronger Maia-1 models, e.g., Maia 1700 failed to make\n",
            "the optimal move. Therefore, in the considered case, as opposed to Maia-1, Maia-2 yields smooth\n",
            "predictions provided that its treatment of this position is monotonic and transitional.\n",
            "14\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 3:\n",
            "Figure 4: Maia-2’s chess concept recognition as a function of skill level, as measured by linear\n",
            "activation probes right before (blue) and after (orange) skill-aware attention. (a) Stockfish overall\n",
            "board evaluation for middle-game positions. (b) Stockfish evaluation of middle-game bonuses and\n",
            "penalties to pieces for white. (c) Does the active player own two bishops? (d) Can the active player\n",
            "capture the opponent’s queen?\n",
            "subtle nuances. We now turn our focus to a critical question: does Maia-2 vary in its ability to capture\n",
            "human chess concepts when given different skill levels? Following the chess concepts probing\n",
            "strategy for AlphaZero [22], we show how Maia-2’s grasp of various concepts varies with skill. The\n",
            "left two plots in Figure 4 show concepts for which Maia-2 clearly distinguishes between skill levels,\n",
            "with higher-skill players paying more attention to them than lower-skill players. These are general\n",
            "board evaluations as given by Stockfish [ 6], or aggregate piece values. Note that pre-skill-aware\n",
            "attention is always flat because by construction it cannot vary with skill, since skill-aware attention has\n",
            "not been applied yet. The two plots on the right depict concepts that live closer to fundamental chess\n",
            "rules, and as such are less dependent on player skill. For skill-dependent concepts, the figures reveal\n",
            "an increasing trend in mastery level after skill-aware attention, aligning with the increase in dedicated\n",
            "skill levels. Meanwhile, the model’s mastery level decreases after passing through the skill-aware\n",
            "attention modules, potentially adjusting for the imperfections of human players. Conversely, the\n",
            "skill-aware attention blocks are not responsive to skill-independent concepts.\n",
            "5 Discussion\n",
            "Human Study. In addition to human move matching, we also consider engagement, another\n",
            "dimension of human study. In particular, we implement a randomized experiment on Lichess:\n",
            "human players challenge our bots, and we randomize whether players play against Maia-1 or Maia-2.\n",
            "Our result is that our higher move-matching and our vastly improved coherence, across all skill\n",
            "levels, come at no cost to human subject engagement, and in fact slightly increase engagement:\n",
            "players rematch Maia-2 almost 1 percentage point more than Maia-1 (41.2% vs. 40.3%). Although\n",
            "engagement is not our main objective, this is further promising evidence that we have achieved a\n",
            "more human-aligned model that coherently captures human style across different skill levels.\n",
            "Ethical Considerations. We believe Maia-2 poses limited risk while offering large potential benefits.\n",
            "Our data is highly aggregated, with almost 1 billion games being used for training, and chess as a\n",
            "domain is generally low-risk. Meanwhile, helping people improve in chess could lead to increased\n",
            "cognitive skills, confidence boosts, and help with general life satisfaction. Our vision is for Maia-2 to\n",
            "power AI partners and training aids; it cannot currently replace skilled human tutors and coaches.\n",
            "Limitation. Our work has limitations. First, we are excited by the applications that Maia-2 will\n",
            "enable, such as more relatable AI partners and AI-powered learning aids, the development of which is\n",
            "out of scope for the current work. Maia-2 does not yet incorporate search, although previous work has\n",
            "demonstrated that with proper regularization it can help improve move prediction performance [26].\n",
            "Relatedly, we group the strongest players in a single bucket, although modeling the very best players\n",
            "in the world remains difficult due to the complexity and depth of their moves.\n",
            "10\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 4:\n",
            "interact with chess positions to produce the moves humans make. Unlike previous models, Maia-2\n",
            "only requires the current board position as input (as opposed to six), which dramatically reduces\n",
            "training time and increases flexibility (e.g. for applying the model in non-game contexts where there\n",
            "may be no 6-board history). In addition to policy and value heads like in previous work, we also add\n",
            "an additional auxiliary information head that helps the model learn a deeper understanding of human\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n",
            "\n",
            "Document 5:\n",
            "important dimension is prediction coherence as skill varies. A central drawback of Maia-1 is that it\n",
            "8\n",
            "Source: /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/maia-2.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the output directory\n",
        "output_dir = '/content/drive/MyDrive/Capstone/Week 5_Advanced_RAG'\n",
        "log_file_path = os.path.join(output_dir, \"retrieval_metrics_log.json\")\n",
        "\n",
        "# Function to measure latency and average context length\n",
        "def measure_retrieval_metrics(retriever_function, query):\n",
        "    start_time = time.time()\n",
        "    retrieved_docs = retriever_function(query) # Execute the retrieval process\n",
        "    end_time = time.time()\n",
        "    latency = end_time - start_time\n",
        "\n",
        "    # Calculate average context length\n",
        "    total_length = sum(len(doc.page_content) for doc in retrieved_docs)\n",
        "    avg_context_length = total_length / len(retrieved_docs) if retrieved_docs else 0\n",
        "\n",
        "    return latency, avg_context_length, retrieved_docs\n",
        "\n",
        "# Measure metrics for the RRF + Rerank + Compression pipeline\n",
        "# We will use the retrieve_and_compress function defined previously\n",
        "# Make sure ensemble_retriever, rerank_documents, and compression_pipeline are defined and accessible\n",
        "try:\n",
        "    latency, avg_context_length, retrieved_docs = measure_retrieval_metrics(\n",
        "        lambda q: retrieve_and_compress(q, ensemble_retriever, rerank_documents, compression_pipeline, top_n_rerank=5),\n",
        "        sample_query # Use the sample query for testing\n",
        "    )\n",
        "\n",
        "    # Prepare the metrics to log\n",
        "    metrics = {\n",
        "        \"retriever\": \"RRF + Rerank + Compression\",\n",
        "        \"query\": sample_query,\n",
        "        \"latency_seconds\": latency,\n",
        "        \"average_context_length\": avg_context_length,\n",
        "        \"num_retrieved_docs\": len(retrieved_docs)\n",
        "    }\n",
        "\n",
        "    # Check if the log file exists and load existing data\n",
        "    existing_logs = []\n",
        "    if os.path.exists(log_file_path):\n",
        "        try:\n",
        "            with open(log_file_path, 'r') as f:\n",
        "                existing_logs = json.load(f)\n",
        "        except json.JSONDecodeError:\n",
        "            existing_logs = [] # Handle empty or invalid JSON\n",
        "\n",
        "    # Append new metrics\n",
        "    existing_logs.append(metrics)\n",
        "\n",
        "    # Save the updated logs to the file\n",
        "    with open(log_file_path, 'w') as f:\n",
        "        json.dump(existing_logs, f, indent=4)\n",
        "\n",
        "    print(f\"\\nLogged retrieval metrics to {log_file_path}\")\n",
        "    print(json.dumps(metrics, indent=4))\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"\\nError: Required variables or functions are not defined. Please ensure ensemble_retriever, rerank_documents, and compression_pipeline are executed before this cell.\")\n",
        "    print(e)\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn unexpected error occurred during metric measurement: {e}\")\n",
        "\n",
        "\n",
        "# Note on Recall@k and Token Cost:\n",
        "# Recall@k requires an evaluation dataset with ground truth relevant documents for each query.\n",
        "# Token cost is relevant if an LLM is used in the pipeline (e.g., for compression/summarization).\n",
        "# To implement these, you would need to:\n",
        "# 1. Create or load an evaluation dataset.\n",
        "# 2. For Recall@k, compare the retrieved documents against the ground truth relevant documents for each query in the dataset.\n",
        "# 3. For Token cost (with LLM), track the token usage of the LLM during the compression step."
      ],
      "metadata": {
        "id": "9xpubacPl3kY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "299ce086-b6be-4f74-a3dc-05ed0abd0ead"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performing RRF retrieval for query: What is the main idea of the Maia-2 paper?\n",
            "Reranking 8 retrieved documents.\n",
            "\n",
            "Reranked documents using cross-encoder/ms-marco-MiniLM-L-6-v2.\n",
            "Applying compression to 5 reranked documents.\n",
            "\n",
            "Logged retrieval metrics to /content/drive/MyDrive/Capstone/Week 5_Advanced_RAG/retrieval_metrics_log.json\n",
            "{\n",
            "    \"retriever\": \"RRF + Rerank + Compression\",\n",
            "    \"query\": \"What is the main idea of the Maia-2 paper?\",\n",
            "    \"latency_seconds\": 1.760443925857544,\n",
            "    \"average_context_length\": 1028.0,\n",
            "    \"num_retrieved_docs\": 5\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "USR5l6zJKxts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a913682"
      },
      "source": [
        "## Step 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41ed7887"
      },
      "source": [
        "## Step 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7db89eb3"
      },
      "source": [
        "## Step 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03001945"
      },
      "source": [
        "## Step 7"
      ]
    }
  ]
}